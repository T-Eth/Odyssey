{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6044afb6-2819-4d96-9937-1806cbc51321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments import BTI_DBF_U\n",
    "import os\n",
    "from functools import partial\n",
    "from datasets import SimpleMNISTDataset, prepare_mnist_data, minmax_normalize\n",
    "import torch\n",
    "from unet import UNet, loss_bti_dbf_paper\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b414313-276d-4a98-b9da-dd1deddd83bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Preparing MNIST dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "clean: 100%|█████████████████████████████| 8000/8000 [00:00<00:00, 14313.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 8000 images to ./MNIST_Data/clean\n",
      "Saved CSV to ./MNIST_Data/clean/clean.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test: 100%|██████████████████████████████| 2000/2000 [00:00<00:00, 14317.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 2000 images to ./MNIST_Data/test\n",
      "Saved CSV to ./MNIST_Data/test/test.csv\n",
      "Test dataset size: 8000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BTI-DBF (U): Models:   0%|                                | 0/5 [00:00<?, ?it/s]/home/tyler/Desktop/ResearchProject/Load_Model.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys are : dict_keys(['net', 'Model Category', 'Architecture_Name', 'Learning_Rate', 'Loss Function', 'optimizer', 'Momentum', 'Weight decay', 'num_workers', 'Pytorch version', 'Clean_test_Loss', 'Train_loss', 'Trigerred_test_loss', 'Trigger type', 'Trigger Size', 'Trigger_location', 'Mapping', 'Normalization Type', 'Mapping Type', 'Dataset', 'Batch Size', 'trigger_fraction', 'test_clean_acc', 'test_trigerred_acc', 'epoch'])\n",
      "==> Building model..\n",
      "The Accuracies on clean samples:   99.475\n",
      "The fooling rate:  100.0\n",
      "Mapping is :  1 <class 'int'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[U] round 0 mask raw init = tensor([[[[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]]]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "[Mask] Epoch 1: Loss=3.9662, Benign Acc=0.1059\n",
      "[Mask] Epoch 2: Loss=3.9669, Benign Acc=0.1071\n",
      "[Mask] Epoch 3: Loss=3.9101, Benign Acc=0.1120\n",
      "[Mask] Epoch 4: Loss=3.8835, Benign Acc=0.1104\n",
      "[Mask] Epoch 5: Loss=3.6432, Benign Acc=0.1293\n",
      "[Mask] Epoch 6: Loss=2.2428, Benign Acc=0.3561\n",
      "[Mask] Epoch 7: Loss=0.4766, Benign Acc=0.8293\n",
      "[Mask] Epoch 8: Loss=0.1800, Benign Acc=0.9315\n",
      "[Mask] Epoch 9: Loss=0.0789, Benign Acc=0.9599\n",
      "[Mask] Epoch 10: Loss=0.0427, Benign Acc=0.9674\n",
      "[Mask] Epoch 11: Loss=0.0186, Benign Acc=0.9734\n",
      "[Mask] Epoch 12: Loss=-0.0078, Benign Acc=0.9760\n",
      "[Mask] Epoch 13: Loss=-0.0483, Benign Acc=0.9819\n",
      "[Mask] Epoch 14: Loss=-0.0859, Benign Acc=0.9839\n",
      "[Mask] Epoch 15: Loss=-0.1615, Benign Acc=0.9855\n",
      "[Mask] Epoch 16: Loss=-0.3378, Benign Acc=0.9876\n",
      "[Mask] Epoch 17: Loss=-0.7983, Benign Acc=0.9901\n",
      "[Mask] Epoch 18: Loss=-2.0137, Benign Acc=0.9886\n",
      "[Mask] Epoch 19: Loss=-3.0402, Benign Acc=0.9902\n",
      "[Mask] Epoch 20: Loss=-3.4782, Benign Acc=0.9896\n",
      "[U] round 0 mask raw trained = tensor([[[[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[0.8000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[0.9000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[0.9000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[0.6000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.9000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:322: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: ./Odysseus-MNIST/Models/Model_895.pth\n",
      "Dataset directory: ./test_results/datasets/Odysseus-MNIST/Models/Model_895.pth_MNIST\n",
      "============================================================\n",
      "Dataset statistics:\n",
      "  Total images: 2000\n",
      "  Triggered images: 1000\n",
      "  Clean images: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:322: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(output)\n",
      "/home/tyler/Desktop/ResearchProject/experiments.py:505: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  results_df = pd.concat(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[U] round 0 mask raw init = tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "[Mask] Epoch 1: Loss=2.4399, Benign Acc=0.1138\n",
      "[Mask] Epoch 2: Loss=2.4319, Benign Acc=0.1138\n",
      "[Mask] Epoch 3: Loss=2.4003, Benign Acc=0.1138\n",
      "[Mask] Epoch 4: Loss=2.2641, Benign Acc=0.1138\n",
      "[Mask] Epoch 5: Loss=1.6076, Benign Acc=0.2725\n",
      "[Mask] Epoch 6: Loss=0.3595, Benign Acc=0.9832\n",
      "[Mask] Epoch 7: Loss=0.0752, Benign Acc=0.9964\n",
      "[Mask] Epoch 8: Loss=0.0405, Benign Acc=0.9981\n",
      "[Mask] Epoch 9: Loss=0.0274, Benign Acc=0.9975\n",
      "[Mask] Epoch 10: Loss=0.0201, Benign Acc=0.9982\n",
      "[Mask] Epoch 11: Loss=0.0158, Benign Acc=0.9982\n",
      "[Mask] Epoch 12: Loss=0.0136, Benign Acc=0.9986\n",
      "[Mask] Epoch 13: Loss=0.0105, Benign Acc=0.9981\n",
      "[Mask] Epoch 14: Loss=0.0074, Benign Acc=0.9984\n",
      "[Mask] Epoch 15: Loss=0.0072, Benign Acc=0.9986\n",
      "[Mask] Epoch 16: Loss=0.0053, Benign Acc=0.9994\n",
      "[Mask] Epoch 17: Loss=0.0041, Benign Acc=0.9999\n",
      "[Mask] Epoch 18: Loss=0.0030, Benign Acc=0.9992\n",
      "[Mask] Epoch 19: Loss=0.0034, Benign Acc=0.9990\n",
      "[Mask] Epoch 20: Loss=0.0019, Benign Acc=0.9998\n",
      "[U] round 0 mask raw trained = tensor([[0.1000, 0.2000, 0.1000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.2000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.0000, 0.1000, 0.2000, 0.1000, 0.1000, 0.1000, 0.4000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2000, 0.5000, 0.2000, 0.1000, 0.1000, 0.1000, 0.0000, 0.1000, 0.2000,\n",
      "         0.2000, 0.1000, 0.1000, 0.1000, 0.2000, 0.1000, 0.3000, 0.2000, 0.1000,\n",
      "         0.1000, 0.0000, 0.5000, 0.1000, 0.2000, 0.5000, 0.1000, 0.0000, 0.4000,\n",
      "         0.0000, 0.0000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0000,\n",
      "         0.0000, 0.2000, 0.4000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.1000, 0.4000, 0.2000, 0.1000, 0.2000, 0.1000,\n",
      "         0.0000, 0.2000, 0.2000, 0.1000, 0.3000, 0.1000, 0.1000, 0.1000, 0.3000,\n",
      "         0.1000, 0.4000, 0.2000, 0.1000, 0.1000, 0.1000, 0.2000, 0.2000, 0.2000,\n",
      "         0.3000, 0.1000, 0.0000, 0.1000, 0.2000, 0.0000, 0.1000, 0.1000, 0.0000,\n",
      "         0.3000, 0.1000, 0.2000, 0.1000, 0.2000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.2000, 0.1000, 0.1000, 0.1000, 0.1000, 0.2000, 0.0000, 0.1000, 0.3000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.2000, 0.1000, 0.1000,\n",
      "         0.0000, 0.2000, 0.0000, 0.1000, 0.0000, 0.1000, 0.1000, 0.0000, 0.1000,\n",
      "         0.2000, 0.3000, 0.1000, 0.2000, 0.1000, 0.1000, 0.1000, 0.2000, 0.1000,\n",
      "         0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.1000, 0.1000, 0.1000, 0.2000, 0.1000, 0.1000, 0.1000, 0.2000,\n",
      "         0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0000, 0.1000, 0.1000, 0.1000,\n",
      "         0.2000, 0.0000, 0.0000, 0.1000, 0.1000, 0.0000, 0.2000, 0.1000, 0.0000,\n",
      "         0.4000, 0.1000, 0.1000, 0.3000, 0.1000, 0.0000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000, 0.2000, 0.1000, 0.1000, 0.2000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2000, 0.2000, 0.0000, 0.1000, 0.2000, 0.0000, 0.2000, 0.3000,\n",
      "         0.1000, 0.2000, 0.3000, 0.4000, 0.1000, 0.0000, 0.3000, 0.2000, 0.1000,\n",
      "         0.1000, 0.2000, 0.1000, 0.1000, 0.2000, 0.2000, 0.1000, 0.2000, 0.1000,\n",
      "         0.1000, 0.3000, 0.1000, 0.0000, 0.1000, 0.1000, 0.1000, 0.0000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1000, 0.2000, 0.1000, 0.2000, 0.1000, 0.0000, 0.1000, 0.1000,\n",
      "         0.4000, 0.1000, 0.1000, 0.2000, 0.3000, 0.1000, 0.1000, 0.4000, 0.2000,\n",
      "         0.0000, 0.1000, 0.2000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0000, 0.1000,\n",
      "         0.0000, 0.0000, 0.1000, 0.1000, 0.1000, 0.0000, 0.1000, 0.2000, 0.2000,\n",
      "         0.1000, 0.1000, 0.1000, 0.0000, 0.1000, 0.0000, 0.1000, 0.1000, 0.0000,\n",
      "         0.1000, 0.1000, 0.1000, 0.2000, 0.0000, 0.1000, 0.1000, 0.0000, 0.1000,\n",
      "         0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000, 0.0000, 0.4000, 0.0000, 0.3000, 0.1000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2000, 0.0000, 0.1000, 0.3000, 0.1000, 0.2000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000, 0.0000, 0.1000, 0.0000, 0.0000, 0.4000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2000, 0.1000, 0.2000, 0.3000, 0.1000, 0.2000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000, 0.1000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0000,\n",
      "         0.1000, 0.1000, 0.0000, 0.1000, 0.1000, 0.1000, 0.4000, 0.1000, 0.1000,\n",
      "         0.2000, 0.1000, 0.1000, 0.1000, 0.1000, 0.3000, 0.2000, 0.1000, 0.1000,\n",
      "         0.1000, 0.4000, 0.1000, 0.1000, 0.2000, 0.0000, 0.1000, 0.1000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.1000, 0.1000,\n",
      "         0.2000, 0.1000, 0.1000, 0.2000, 0.0000, 0.0000, 0.4000, 0.5000, 0.4000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2000, 0.1000, 0.1000, 0.0000, 0.1000, 0.1000, 0.2000, 0.1000, 0.1000,\n",
      "         0.1000, 0.0000, 0.1000, 0.0000, 0.2000, 0.1000, 0.1000, 0.2000, 0.2000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.2000, 0.1000,\n",
      "         0.1000, 0.1000, 0.2000, 0.1000, 0.0000, 0.0000, 0.1000, 0.1000, 0.0000,\n",
      "         0.1000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.2000, 0.1000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.1000,\n",
      "         0.1000, 0.0000, 0.0000, 0.2000, 0.1000, 0.0000, 0.1000, 0.1000, 0.1000,\n",
      "         0.0000, 0.1000, 0.1000, 0.0000, 0.0000, 0.1000, 0.2000, 0.5000, 0.1000,\n",
      "         0.1000, 0.1000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0000, 0.0000,\n",
      "         0.1000, 0.1000, 0.0000, 0.1000, 0.0000, 0.3000, 0.1000, 0.2000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.1000,\n",
      "         0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.3000, 0.1000,\n",
      "         0.0000, 0.2000, 0.1000, 0.1000, 0.1000, 0.2000, 0.1000, 0.0000, 0.0000,\n",
      "         0.2000, 0.1000, 0.4000, 0.2000, 0.1000, 0.0000, 0.2000, 0.2000, 0.1000,\n",
      "         0.0000, 0.3000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.2000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.1000, 0.0000, 0.3000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.4000, 0.2000, 0.2000, 0.1000, 0.3000, 0.1000, 0.0000, 0.1000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2000, 0.3000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.2000,\n",
      "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.2000, 0.2000, 0.2000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.0000, 0.4000, 0.1000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1000, 0.2000, 0.1000, 0.1000, 0.2000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000, 0.2000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.2000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.1000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.2000,\n",
      "         0.0000, 0.4000, 0.1000, 0.0000, 0.1000, 0.3000, 0.1000, 0.1000, 0.2000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.1000, 0.2000, 0.1000, 0.1000, 0.1000, 0.2000, 0.1000, 0.2000,\n",
      "         0.2000, 0.1000, 0.0000, 0.2000, 0.3000, 0.2000, 0.1000, 0.2000, 0.1000,\n",
      "         0.1000, 0.1000, 0.0000, 0.1000, 0.2000, 0.5000, 0.1000, 0.1000, 0.2000,\n",
      "         0.1000, 0.3000, 0.2000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.0000, 0.1000, 0.3000, 0.2000, 0.1000, 0.3000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.1000, 0.2000, 0.4000, 0.1000, 0.1000, 0.1000, 0.3000, 0.2000,\n",
      "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.1000, 0.1000, 0.0000, 0.1000, 0.1000, 0.0000, 0.4000, 0.2000,\n",
      "         0.2000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.2000, 0.1000, 0.3000,\n",
      "         0.1000, 0.1000, 0.2000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.2000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.5000, 0.1000, 0.1000, 0.0000,\n",
      "         0.2000, 0.1000, 0.3000, 0.1000, 0.1000, 0.1000, 0.1000, 0.3000, 0.2000,\n",
      "         0.0000, 0.1000, 0.0000, 0.0000, 0.3000, 0.0000, 0.3000, 0.3000, 0.0000,\n",
      "         0.1000, 0.1000, 0.1000, 0.0000, 0.0000, 0.1000, 0.1000, 0.1000, 0.0000,\n",
      "         0.1000, 0.1000, 0.0000, 0.1000, 0.2000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000, 0.3000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000, 0.1000, 0.1000, 0.2000, 0.1000, 0.1000, 0.1000, 0.0000, 0.1000,\n",
      "         0.1000, 0.2000, 0.0000, 0.1000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.2000, 0.2000, 0.1000, 0.1000, 0.2000, 0.2000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000, 0.1000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000, 0.1000, 0.1000, 0.0000, 0.1000, 0.3000, 0.2000, 0.2000, 0.1000,\n",
      "         0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.1000,\n",
      "         0.0000, 0.1000, 0.1000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.2000,\n",
      "         0.0000, 0.0000, 0.0000, 0.2000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000, 0.1000, 0.1000, 0.0000, 0.3000, 0.3000, 0.2000, 0.3000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.4000, 0.1000, 0.2000, 0.1000, 0.1000, 0.4000, 0.2000, 0.3000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.1000, 0.2000, 0.0000, 0.2000, 0.2000, 0.1000, 0.1000, 0.2000,\n",
      "         0.1000, 0.1000, 0.2000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.3000, 0.1000, 0.0000, 0.2000, 0.1000, 0.2000, 0.1000, 0.1000, 0.2000,\n",
      "         0.0000, 0.1000, 0.3000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:322: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: ./Odysseus-MNIST/Models/Model_895.pth\n",
      "Dataset directory: ./test_results/datasets/Odysseus-MNIST/Models/Model_895.pth_MNIST\n",
      "============================================================\n",
      "Dataset statistics:\n",
      "  Total images: 2000\n",
      "  Triggered images: 1000\n",
      "  Clean images: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:322: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(output)\n",
      "BTI-DBF (U): Models:  20%|████▊                   | 1/5 [01:17<05:10, 77.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys are : dict_keys(['net', 'Model Category', 'Architecture_Name', 'Learning_Rate', 'Loss Function', 'optimizer', 'Momentum', 'Weight decay', 'num_workers', 'Pytorch version', 'Clean_test_Loss', 'Train_loss', 'Trigerred_test_loss', 'Trigger type', 'Trigger Size', 'Trigger_location', 'Mapping', 'Normalization Type', 'Mapping Type', 'Dataset', 'Batch Size', 'trigger_fraction', 'test_clean_acc', 'test_trigerred_acc', 'epoch'])\n",
      "==> Building model..\n",
      "The Accuracies on clean samples:   99.5\n",
      "The fooling rate:  100.0\n",
      "Mapping is :  3 <class 'int'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n",
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[U] round 0 mask raw init = tensor([[[[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]]]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "[Mask] Epoch 1: Loss=3.6551, Benign Acc=0.0980\n",
      "[Mask] Epoch 2: Loss=3.5900, Benign Acc=0.1019\n",
      "[Mask] Epoch 3: Loss=3.5681, Benign Acc=0.1079\n",
      "[Mask] Epoch 4: Loss=3.3065, Benign Acc=0.1364\n",
      "[Mask] Epoch 5: Loss=2.1662, Benign Acc=0.3435\n",
      "[Mask] Epoch 6: Loss=0.4098, Benign Acc=0.8632\n",
      "[Mask] Epoch 7: Loss=0.1053, Benign Acc=0.9627\n",
      "[Mask] Epoch 8: Loss=0.0564, Benign Acc=0.9770\n",
      "[Mask] Epoch 9: Loss=0.0396, Benign Acc=0.9811\n",
      "[Mask] Epoch 10: Loss=0.0320, Benign Acc=0.9826\n",
      "[Mask] Epoch 11: Loss=0.0217, Benign Acc=0.9856\n",
      "[Mask] Epoch 12: Loss=0.0185, Benign Acc=0.9872\n",
      "[Mask] Epoch 13: Loss=0.0196, Benign Acc=0.9865\n",
      "[Mask] Epoch 14: Loss=0.0142, Benign Acc=0.9871\n",
      "[Mask] Epoch 15: Loss=0.0102, Benign Acc=0.9905\n",
      "[Mask] Epoch 16: Loss=0.0063, Benign Acc=0.9890\n",
      "[Mask] Epoch 17: Loss=0.0087, Benign Acc=0.9880\n",
      "[Mask] Epoch 18: Loss=0.0064, Benign Acc=0.9884\n",
      "[Mask] Epoch 19: Loss=0.0080, Benign Acc=0.9885\n",
      "[Mask] Epoch 20: Loss=0.0077, Benign Acc=0.9886\n",
      "[U] round 0 mask raw trained = tensor([[[[0.1000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:421: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: ./Odysseus-MNIST/Models/Model_749.pth\n",
      "Dataset directory: ./test_results/datasets/Odysseus-MNIST/Models/Model_749.pth_MNIST\n",
      "============================================================\n",
      "Dataset statistics:\n",
      "  Total images: 2000\n",
      "  Triggered images: 1000\n",
      "  Clean images: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:421: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[U] round 0 mask raw init = tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "[Mask] Epoch 1: Loss=2.1985, Benign Acc=0.0999\n",
      "[Mask] Epoch 2: Loss=2.0377, Benign Acc=0.1001\n",
      "[Mask] Epoch 3: Loss=1.5638, Benign Acc=0.5115\n",
      "[Mask] Epoch 4: Loss=0.6288, Benign Acc=0.9794\n",
      "[Mask] Epoch 5: Loss=0.1543, Benign Acc=0.9928\n",
      "[Mask] Epoch 6: Loss=0.0691, Benign Acc=0.9954\n",
      "[Mask] Epoch 7: Loss=0.0429, Benign Acc=0.9956\n",
      "[Mask] Epoch 8: Loss=0.0307, Benign Acc=0.9960\n",
      "[Mask] Epoch 9: Loss=0.0231, Benign Acc=0.9960\n",
      "[Mask] Epoch 10: Loss=0.0173, Benign Acc=0.9965\n",
      "[Mask] Epoch 11: Loss=0.0135, Benign Acc=0.9969\n",
      "[Mask] Epoch 12: Loss=0.0100, Benign Acc=0.9971\n",
      "[Mask] Epoch 13: Loss=0.0085, Benign Acc=0.9972\n",
      "[Mask] Epoch 14: Loss=0.0064, Benign Acc=0.9971\n",
      "[Mask] Epoch 15: Loss=0.0047, Benign Acc=0.9972\n",
      "[Mask] Epoch 16: Loss=0.0046, Benign Acc=0.9974\n",
      "[Mask] Epoch 17: Loss=0.0025, Benign Acc=0.9970\n",
      "[Mask] Epoch 18: Loss=0.0017, Benign Acc=0.9972\n",
      "[Mask] Epoch 19: Loss=0.0007, Benign Acc=0.9975\n",
      "[Mask] Epoch 20: Loss=-0.0002, Benign Acc=0.9975\n",
      "[U] round 0 mask raw trained = tensor([[0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:421: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: ./Odysseus-MNIST/Models/Model_749.pth\n",
      "Dataset directory: ./test_results/datasets/Odysseus-MNIST/Models/Model_749.pth_MNIST\n",
      "============================================================\n",
      "Dataset statistics:\n",
      "  Total images: 2000\n",
      "  Triggered images: 1000\n",
      "  Clean images: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:421: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(output)\n",
      "BTI-DBF (U): Models:  40%|█████████▌              | 2/5 [02:37<03:57, 79.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys are : dict_keys(['net', 'Model Category', 'Architecture_Name', 'Learning_Rate', 'Loss Function', 'optimizer', 'Momentum', 'Weight decay', 'num_workers', 'Pytorch version', 'Clean_test_Loss', 'Train_loss', 'Trigerred_test_loss', 'Trigger type', 'Trigger Size', 'Trigger_location', 'Mapping', 'Normalization Type', 'Mapping Type', 'Dataset', 'Batch Size', 'trigger_fraction', 'test_clean_acc', 'test_trigerred_acc', 'epoch'])\n",
      "==> Building model..\n",
      "The Accuracies on clean samples:   99.50588235294117\n",
      "The fooling rate:  100.0\n",
      "Mapping is :  1 <class 'int'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n",
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[U] round 0 mask raw init = tensor([[[[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]]]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "[Mask] Epoch 1: Loss=3.2738, Benign Acc=0.1021\n",
      "[Mask] Epoch 2: Loss=3.2559, Benign Acc=0.1056\n",
      "[Mask] Epoch 3: Loss=3.1660, Benign Acc=0.1209\n",
      "[Mask] Epoch 4: Loss=2.9079, Benign Acc=0.1507\n",
      "[Mask] Epoch 5: Loss=1.8499, Benign Acc=0.3916\n",
      "[Mask] Epoch 6: Loss=0.4093, Benign Acc=0.8686\n",
      "[Mask] Epoch 7: Loss=0.1211, Benign Acc=0.9636\n",
      "[Mask] Epoch 8: Loss=0.0636, Benign Acc=0.9794\n",
      "[Mask] Epoch 9: Loss=0.0462, Benign Acc=0.9851\n",
      "[Mask] Epoch 10: Loss=0.0325, Benign Acc=0.9851\n",
      "[Mask] Epoch 11: Loss=0.0312, Benign Acc=0.9864\n",
      "[Mask] Epoch 12: Loss=0.0265, Benign Acc=0.9870\n",
      "[Mask] Epoch 13: Loss=0.0211, Benign Acc=0.9891\n",
      "[Mask] Epoch 14: Loss=0.0194, Benign Acc=0.9900\n",
      "[Mask] Epoch 15: Loss=0.0183, Benign Acc=0.9882\n",
      "[Mask] Epoch 16: Loss=0.0133, Benign Acc=0.9906\n",
      "[Mask] Epoch 17: Loss=0.0128, Benign Acc=0.9900\n",
      "[Mask] Epoch 18: Loss=0.0117, Benign Acc=0.9910\n",
      "[Mask] Epoch 19: Loss=0.0084, Benign Acc=0.9901\n",
      "[Mask] Epoch 20: Loss=0.0083, Benign Acc=0.9902\n",
      "[U] round 0 mask raw trained = tensor([[[[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.3000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:421: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: ./Odysseus-MNIST/Models/Model_752.pth\n",
      "Dataset directory: ./test_results/datasets/Odysseus-MNIST/Models/Model_752.pth_MNIST\n",
      "============================================================\n",
      "Dataset statistics:\n",
      "  Total images: 2000\n",
      "  Triggered images: 1000\n",
      "  Clean images: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:421: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[U] round 0 mask raw init = tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "[Mask] Epoch 1: Loss=2.1827, Benign Acc=0.1138\n",
      "[Mask] Epoch 2: Loss=2.0381, Benign Acc=0.1161\n",
      "[Mask] Epoch 3: Loss=1.6085, Benign Acc=0.6469\n",
      "[Mask] Epoch 4: Loss=0.7001, Benign Acc=0.9890\n",
      "[Mask] Epoch 5: Loss=0.1755, Benign Acc=0.9954\n",
      "[Mask] Epoch 6: Loss=0.0746, Benign Acc=0.9968\n",
      "[Mask] Epoch 7: Loss=0.0465, Benign Acc=0.9964\n",
      "[Mask] Epoch 8: Loss=0.0328, Benign Acc=0.9968\n",
      "[Mask] Epoch 9: Loss=0.0248, Benign Acc=0.9972\n",
      "[Mask] Epoch 10: Loss=0.0204, Benign Acc=0.9975\n",
      "[Mask] Epoch 11: Loss=0.0170, Benign Acc=0.9976\n",
      "[Mask] Epoch 12: Loss=0.0137, Benign Acc=0.9974\n",
      "[Mask] Epoch 13: Loss=0.0118, Benign Acc=0.9976\n",
      "[Mask] Epoch 14: Loss=0.0098, Benign Acc=0.9976\n",
      "[Mask] Epoch 15: Loss=0.0080, Benign Acc=0.9985\n",
      "[Mask] Epoch 16: Loss=0.0079, Benign Acc=0.9982\n",
      "[Mask] Epoch 17: Loss=0.0065, Benign Acc=0.9986\n",
      "[Mask] Epoch 18: Loss=0.0058, Benign Acc=0.9981\n",
      "[Mask] Epoch 19: Loss=0.0044, Benign Acc=0.9989\n",
      "[Mask] Epoch 20: Loss=0.0042, Benign Acc=0.9982\n",
      "[U] round 0 mask raw trained = tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:421: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: ./Odysseus-MNIST/Models/Model_752.pth\n",
      "Dataset directory: ./test_results/datasets/Odysseus-MNIST/Models/Model_752.pth_MNIST\n",
      "============================================================\n",
      "Dataset statistics:\n",
      "  Total images: 2000\n",
      "  Triggered images: 1000\n",
      "  Clean images: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:421: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(output)\n",
      "BTI-DBF (U): Models:  60%|██████████████▍         | 3/5 [03:54<02:35, 77.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys are : dict_keys(['net', 'Model Category', 'Architecture_Name', 'Learning_Rate', 'Loss Function', 'optimizer', 'Momentum', 'Weight decay', 'num_workers', 'Pytorch version', 'Clean_test_Loss', 'Train_loss', 'Trigerred_test_loss', 'Trigger type', 'Trigger Size', 'Trigger_location', 'Mapping', 'Normalization Type', 'Mapping Type', 'Dataset', 'Batch Size', 'trigger_fraction', 'test_clean_acc', 'test_trigerred_acc', 'epoch'])\n",
      "==> Building model..\n",
      "The Accuracies on clean samples:   99.3625\n",
      "The fooling rate:  99.8\n",
      "Mapping is :  1 <class 'int'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n",
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[U] round 0 mask raw init = tensor([[[[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]]]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "[Mask] Epoch 1: Loss=3.8609, Benign Acc=0.1057\n",
      "[Mask] Epoch 2: Loss=3.8199, Benign Acc=0.1123\n",
      "[Mask] Epoch 3: Loss=3.7549, Benign Acc=0.1153\n",
      "[Mask] Epoch 4: Loss=3.3509, Benign Acc=0.1475\n",
      "[Mask] Epoch 5: Loss=1.8264, Benign Acc=0.4235\n",
      "[Mask] Epoch 6: Loss=0.3904, Benign Acc=0.8622\n",
      "[Mask] Epoch 7: Loss=0.1392, Benign Acc=0.9495\n",
      "[Mask] Epoch 8: Loss=0.0752, Benign Acc=0.9714\n",
      "[Mask] Epoch 9: Loss=0.0512, Benign Acc=0.9762\n",
      "[Mask] Epoch 10: Loss=0.0385, Benign Acc=0.9809\n",
      "[Mask] Epoch 11: Loss=0.0266, Benign Acc=0.9832\n",
      "[Mask] Epoch 12: Loss=0.0255, Benign Acc=0.9842\n",
      "[Mask] Epoch 13: Loss=0.0199, Benign Acc=0.9871\n",
      "[Mask] Epoch 14: Loss=0.0185, Benign Acc=0.9851\n",
      "[Mask] Epoch 15: Loss=0.0171, Benign Acc=0.9870\n",
      "[Mask] Epoch 16: Loss=0.0122, Benign Acc=0.9872\n",
      "[Mask] Epoch 17: Loss=0.0118, Benign Acc=0.9871\n",
      "[Mask] Epoch 18: Loss=0.0101, Benign Acc=0.9882\n",
      "[Mask] Epoch 19: Loss=0.0077, Benign Acc=0.9889\n",
      "[Mask] Epoch 20: Loss=0.0086, Benign Acc=0.9885\n",
      "[U] round 0 mask raw trained = tensor([[[[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:371: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: ./Odysseus-MNIST/Models/Model_957.pth\n",
      "Dataset directory: ./test_results/datasets/Odysseus-MNIST/Models/Model_957.pth_MNIST\n",
      "============================================================\n",
      "Dataset statistics:\n",
      "  Total images: 2000\n",
      "  Triggered images: 1000\n",
      "  Clean images: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:371: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[U] round 0 mask raw init = tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0',\n",
      "       grad_fn=<DivBackward0>)\n",
      "[Mask] Epoch 1: Loss=2.5259, Benign Acc=0.1138\n",
      "[Mask] Epoch 2: Loss=2.3493, Benign Acc=0.1138\n",
      "[Mask] Epoch 3: Loss=1.8068, Benign Acc=0.1216\n",
      "[Mask] Epoch 4: Loss=0.6727, Benign Acc=0.7552\n",
      "[Mask] Epoch 5: Loss=0.1410, Benign Acc=0.9950\n",
      "[Mask] Epoch 6: Loss=0.0611, Benign Acc=0.9978\n",
      "[Mask] Epoch 7: Loss=0.0394, Benign Acc=0.9980\n",
      "[Mask] Epoch 8: Loss=0.0293, Benign Acc=0.9980\n",
      "[Mask] Epoch 9: Loss=0.0232, Benign Acc=0.9978\n",
      "[Mask] Epoch 10: Loss=0.0191, Benign Acc=0.9981\n",
      "[Mask] Epoch 11: Loss=0.0163, Benign Acc=0.9980\n",
      "[Mask] Epoch 12: Loss=0.0137, Benign Acc=0.9981\n",
      "[Mask] Epoch 13: Loss=0.0122, Benign Acc=0.9984\n",
      "[Mask] Epoch 14: Loss=0.0111, Benign Acc=0.9978\n",
      "[Mask] Epoch 15: Loss=0.0097, Benign Acc=0.9985\n",
      "[Mask] Epoch 16: Loss=0.0091, Benign Acc=0.9980\n",
      "[Mask] Epoch 17: Loss=0.0079, Benign Acc=0.9980\n",
      "[Mask] Epoch 18: Loss=0.0073, Benign Acc=0.9986\n",
      "[Mask] Epoch 19: Loss=0.0065, Benign Acc=0.9984\n",
      "[Mask] Epoch 20: Loss=0.0064, Benign Acc=0.9982\n",
      "[U] round 0 mask raw trained = tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:371: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: ./Odysseus-MNIST/Models/Model_957.pth\n",
      "Dataset directory: ./test_results/datasets/Odysseus-MNIST/Models/Model_957.pth_MNIST\n",
      "============================================================\n",
      "Dataset statistics:\n",
      "  Total images: 2000\n",
      "  Triggered images: 1000\n",
      "  Clean images: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:371: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(output)\n",
      "BTI-DBF (U): Models:  80%|███████████████████▏    | 4/5 [05:05<01:15, 75.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys are : dict_keys(['net', 'Model Category', 'Architecture_Name', 'Learning_Rate', 'Loss Function', 'optimizer', 'Momentum', 'Weight decay', 'num_workers', 'Pytorch version', 'Clean_test_Loss', 'Train_loss', 'Trigerred_test_loss', 'Trigger type', 'Trigger Size', 'Trigger_location', 'Mapping', 'Normalization Type', 'Mapping Type', 'Dataset', 'Batch Size', 'trigger_fraction', 'test_clean_acc', 'test_trigerred_acc', 'epoch'])\n",
      "==> Building model..\n",
      "The Accuracies on clean samples:   99.3125\n",
      "The fooling rate:  100.0\n",
      "Mapping is :  5 <class 'int'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n",
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[U] round 0 mask raw init = tensor([[[[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]]]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "[Mask] Epoch 1: Loss=3.9553, Benign Acc=0.0998\n",
      "[Mask] Epoch 2: Loss=3.9809, Benign Acc=0.1010\n",
      "[Mask] Epoch 3: Loss=3.9423, Benign Acc=0.1030\n",
      "[Mask] Epoch 4: Loss=3.9286, Benign Acc=0.0973\n",
      "[Mask] Epoch 5: Loss=3.6345, Benign Acc=0.1226\n",
      "[Mask] Epoch 6: Loss=2.0597, Benign Acc=0.3800\n",
      "[Mask] Epoch 7: Loss=0.3181, Benign Acc=0.8821\n",
      "[Mask] Epoch 8: Loss=0.0943, Benign Acc=0.9611\n",
      "[Mask] Epoch 9: Loss=0.0533, Benign Acc=0.9756\n",
      "[Mask] Epoch 10: Loss=0.0406, Benign Acc=0.9769\n",
      "[Mask] Epoch 11: Loss=0.0247, Benign Acc=0.9812\n",
      "[Mask] Epoch 12: Loss=0.0217, Benign Acc=0.9845\n",
      "[Mask] Epoch 13: Loss=0.0164, Benign Acc=0.9835\n",
      "[Mask] Epoch 14: Loss=0.0087, Benign Acc=0.9830\n",
      "[Mask] Epoch 15: Loss=0.0028, Benign Acc=0.9864\n",
      "[Mask] Epoch 16: Loss=-0.0006, Benign Acc=0.9856\n",
      "[Mask] Epoch 17: Loss=-0.0084, Benign Acc=0.9869\n",
      "[Mask] Epoch 18: Loss=-0.0149, Benign Acc=0.9871\n",
      "[Mask] Epoch 19: Loss=-0.0164, Benign Acc=0.9870\n",
      "[Mask] Epoch 20: Loss=-0.0282, Benign Acc=0.9876\n",
      "[U] round 0 mask raw trained = tensor([[[[0.2000]],\n",
      "\n",
      "         [[0.5000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.4000]],\n",
      "\n",
      "         [[0.6000]],\n",
      "\n",
      "         [[0.3000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.7000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.6000]],\n",
      "\n",
      "         [[0.6000]],\n",
      "\n",
      "         [[0.8000]],\n",
      "\n",
      "         [[0.4000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.6000]],\n",
      "\n",
      "         [[0.4000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.6000]],\n",
      "\n",
      "         [[0.7000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.4000]],\n",
      "\n",
      "         [[0.6000]],\n",
      "\n",
      "         [[0.7000]],\n",
      "\n",
      "         [[0.5000]],\n",
      "\n",
      "         [[0.4000]],\n",
      "\n",
      "         [[0.6000]],\n",
      "\n",
      "         [[0.5000]],\n",
      "\n",
      "         [[0.6000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.7000]],\n",
      "\n",
      "         [[0.7000]],\n",
      "\n",
      "         [[0.3000]],\n",
      "\n",
      "         [[0.5000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.3000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.5000]],\n",
      "\n",
      "         [[0.7000]],\n",
      "\n",
      "         [[0.8000]],\n",
      "\n",
      "         [[0.5000]],\n",
      "\n",
      "         [[0.5000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.4000]],\n",
      "\n",
      "         [[0.3000]],\n",
      "\n",
      "         [[0.4000]],\n",
      "\n",
      "         [[0.7000]],\n",
      "\n",
      "         [[0.7000]],\n",
      "\n",
      "         [[0.5000]],\n",
      "\n",
      "         [[0.4000]],\n",
      "\n",
      "         [[0.7000]],\n",
      "\n",
      "         [[0.6000]],\n",
      "\n",
      "         [[0.3000]],\n",
      "\n",
      "         [[0.5000]],\n",
      "\n",
      "         [[0.5000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.6000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.7000]],\n",
      "\n",
      "         [[0.5000]],\n",
      "\n",
      "         [[0.6000]]]], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:322: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: ./Odysseus-MNIST/Models/Model_901.pth\n",
      "Dataset directory: ./test_results/datasets/Odysseus-MNIST/Models/Model_901.pth_MNIST\n",
      "============================================================\n",
      "Dataset statistics:\n",
      "  Total images: 2000\n",
      "  Triggered images: 1000\n",
      "  Clean images: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:322: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[U] round 0 mask raw init = tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "[Mask] Epoch 1: Loss=2.4571, Benign Acc=0.0885\n",
      "[Mask] Epoch 2: Loss=2.4447, Benign Acc=0.0885\n",
      "[Mask] Epoch 3: Loss=2.4012, Benign Acc=0.0885\n",
      "[Mask] Epoch 4: Loss=2.2137, Benign Acc=0.0885\n",
      "[Mask] Epoch 5: Loss=1.3916, Benign Acc=0.4189\n",
      "[Mask] Epoch 6: Loss=0.2574, Benign Acc=0.9888\n",
      "[Mask] Epoch 7: Loss=0.0677, Benign Acc=0.9965\n",
      "[Mask] Epoch 8: Loss=0.0383, Benign Acc=0.9966\n",
      "[Mask] Epoch 9: Loss=0.0263, Benign Acc=0.9976\n",
      "[Mask] Epoch 10: Loss=0.0192, Benign Acc=0.9980\n",
      "[Mask] Epoch 11: Loss=0.0144, Benign Acc=0.9981\n",
      "[Mask] Epoch 12: Loss=0.0117, Benign Acc=0.9982\n",
      "[Mask] Epoch 13: Loss=0.0088, Benign Acc=0.9986\n",
      "[Mask] Epoch 14: Loss=0.0067, Benign Acc=0.9986\n",
      "[Mask] Epoch 15: Loss=0.0055, Benign Acc=0.9982\n",
      "[Mask] Epoch 16: Loss=0.0034, Benign Acc=0.9988\n",
      "[Mask] Epoch 17: Loss=0.0032, Benign Acc=0.9991\n",
      "[Mask] Epoch 18: Loss=0.0026, Benign Acc=0.9991\n",
      "[Mask] Epoch 19: Loss=0.0009, Benign Acc=0.9989\n",
      "[Mask] Epoch 20: Loss=0.0002, Benign Acc=0.9992\n",
      "[U] round 0 mask raw trained = tensor([[0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.3000, 0.0000, 0.2000,\n",
      "         0.1000, 0.0000, 0.1000, 0.1000, 0.2000, 0.0000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000, 0.1000, 0.1000, 0.0000, 0.1000, 0.0000, 0.2000, 0.1000, 0.1000,\n",
      "         0.1000, 0.0000, 0.1000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0000,\n",
      "         0.2000, 0.1000, 0.1000, 0.2000, 0.1000, 0.2000, 0.1000, 0.0000, 0.1000,\n",
      "         0.2000, 0.2000, 0.0000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1000, 0.0000, 0.1000, 0.1000, 0.1000, 0.0000, 0.1000, 0.0000,\n",
      "         0.1000, 0.4000, 0.2000, 0.0000, 0.2000, 0.0000, 0.1000, 0.3000, 0.2000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2000, 0.1000, 0.1000, 0.1000, 0.1000, 0.4000, 0.1000, 0.1000, 0.2000,\n",
      "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0000, 0.1000, 0.1000,\n",
      "         0.1000, 0.0000, 0.1000, 0.1000, 0.2000, 0.0000, 0.1000, 0.1000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.2000, 0.1000, 0.0000, 0.3000, 0.0000, 0.1000,\n",
      "         0.1000, 0.2000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0000, 0.0000,\n",
      "         0.1000, 0.2000, 0.1000, 0.4000, 0.1000, 0.1000, 0.0000, 0.2000, 0.0000,\n",
      "         0.1000, 0.1000, 0.2000, 0.0000, 0.3000, 0.1000, 0.0000, 0.2000, 0.3000,\n",
      "         0.1000, 0.2000, 0.1000, 0.1000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000, 0.0000, 0.1000, 0.1000, 0.1000, 0.0000, 0.1000, 0.0000, 0.2000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2000, 0.2000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.2000, 0.2000,\n",
      "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0000, 0.1000, 0.1000,\n",
      "         0.1000, 0.2000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.1000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1000, 0.2000, 0.0000, 0.0000, 0.1000, 0.0000, 0.1000, 0.1000,\n",
      "         0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4000, 0.0000,\n",
      "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.2000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000, 0.1000, 0.1000, 0.2000, 0.1000, 0.2000, 0.1000, 0.1000, 0.3000,\n",
      "         0.1000, 0.0000, 0.1000, 0.1000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.0000, 0.1000, 0.1000, 0.0000, 0.1000, 0.0000, 0.1000, 0.0000, 0.1000,\n",
      "         0.2000, 0.2000, 0.2000, 0.2000, 0.1000, 0.3000, 0.1000, 0.1000, 0.1000,\n",
      "         0.2000, 0.3000, 0.1000, 0.1000, 0.1000, 0.1000, 0.2000, 0.0000, 0.1000,\n",
      "         0.1000, 0.1000, 0.1000, 0.1000, 0.0000, 0.1000, 0.1000, 0.1000, 0.0000,\n",
      "         0.1000, 0.1000, 0.1000, 0.1000, 0.0000, 0.2000, 0.0000, 0.1000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.2000,\n",
      "         0.1000, 0.2000, 0.0000, 0.2000, 0.0000, 0.1000, 0.1000, 0.3000, 0.3000,\n",
      "         0.2000, 0.1000, 0.1000, 0.1000, 0.0000, 0.1000, 0.1000, 0.3000, 0.1000,\n",
      "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0000, 0.1000, 0.1000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.1000, 0.1000, 0.2000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1000, 0.1000, 0.0000, 0.0000, 0.1000, 0.1000, 0.0000, 0.1000,\n",
      "         0.0000, 0.4000, 0.1000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000, 0.1000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.1000,\n",
      "         0.0000, 0.1000, 0.1000, 0.1000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.0000, 0.2000, 0.1000, 0.1000, 0.2000, 0.2000, 0.1000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2000, 0.1000, 0.0000, 0.0000, 0.0000, 0.2000, 0.3000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.1000, 0.2000, 0.0000, 0.1000, 0.0000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.3000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0000,\n",
      "         0.2000, 0.0000, 0.1000, 0.0000, 0.2000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000, 0.2000, 0.4000, 0.1000, 0.3000, 0.1000, 0.1000, 0.4000, 0.1000,\n",
      "         0.3000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.1000, 0.0000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.5000, 0.0000, 0.0000, 0.1000, 0.1000, 0.2000, 0.1000, 0.1000,\n",
      "         0.0000, 0.1000, 0.1000, 0.3000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000, 0.1000, 0.2000, 0.1000, 0.1000, 0.4000, 0.1000, 0.1000, 0.0000,\n",
      "         0.1000, 0.3000, 0.2000, 0.0000, 0.1000, 0.1000, 0.0000, 0.1000, 0.0000,\n",
      "         0.1000, 0.1000, 0.1000, 0.0000, 0.1000, 0.1000, 0.2000, 0.2000, 0.0000,\n",
      "         0.1000, 0.2000, 0.1000, 0.0000, 0.0000, 0.4000, 0.1000, 0.2000, 0.1000,\n",
      "         0.1000, 0.7000, 0.2000, 0.1000, 0.1000, 0.4000, 0.2000, 0.1000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.1000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.2000, 0.1000,\n",
      "         0.1000, 0.0000, 0.2000, 0.1000, 0.1000, 0.1000, 0.0000, 0.1000, 0.0000,\n",
      "         0.4000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0000, 0.1000,\n",
      "         0.1000, 0.1000, 0.2000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0000, 0.0000, 0.1000,\n",
      "         0.1000, 0.1000, 0.1000, 0.1000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.1000, 0.0000, 0.1000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2000, 0.2000, 0.0000, 0.1000, 0.1000, 0.0000, 0.1000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1000, 0.0000, 0.1000, 0.1000, 0.2000, 0.0000, 0.0000, 0.1000,\n",
      "         0.2000, 0.3000, 0.1000, 0.1000, 0.2000, 0.0000, 0.1000, 0.1000, 0.3000,\n",
      "         0.1000, 0.2000, 0.1000, 0.1000, 0.0000, 0.0000, 0.1000, 0.0000, 0.1000,\n",
      "         0.1000, 0.1000, 0.5000, 0.1000, 0.1000, 0.2000, 0.1000, 0.0000, 0.1000,\n",
      "         0.1000, 0.2000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.1000,\n",
      "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.1000, 0.1000, 0.0000,\n",
      "         0.1000, 0.1000, 0.2000, 0.2000, 0.0000, 0.1000, 0.2000, 0.1000, 0.0000,\n",
      "         0.0000, 0.1000, 0.1000, 0.0000, 0.1000, 0.0000, 0.0000, 0.1000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.1000, 0.2000, 0.2000, 0.0000, 0.1000, 0.1000, 0.0000, 0.2000,\n",
      "         0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.2000, 0.1000, 0.1000, 0.0000,\n",
      "         0.0000, 0.1000, 0.1000, 0.0000, 0.1000, 0.1000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.5000, 0.1000, 0.1000, 0.1000,\n",
      "         0.2000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000, 0.2000, 0.1000, 0.1000, 0.1000, 0.1000, 0.2000, 0.1000, 0.1000,\n",
      "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.2000, 0.2000, 0.1000, 0.3000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.1000, 0.1000, 0.1000, 0.0000, 0.0000, 0.1000, 0.1000, 0.0000,\n",
      "         0.0000, 0.3000, 0.2000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.1000, 0.0000, 0.1000, 0.1000, 0.2000, 0.1000, 0.1000, 0.1000,\n",
      "         0.0000, 0.1000, 0.1000, 0.1000, 0.0000, 0.1000, 0.1000, 0.5000, 0.2000,\n",
      "         0.0000, 0.1000, 0.1000, 0.0000, 0.1000, 0.1000, 0.1000, 0.0000, 0.1000,\n",
      "         0.1000, 0.1000, 0.1000, 0.2000, 0.1000, 0.1000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.1000,\n",
      "         0.1000, 0.0000, 0.2000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.2000, 0.1000, 0.1000, 0.0000,\n",
      "         0.0000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0000, 0.1000, 0.0000,\n",
      "         0.1000, 0.0000, 0.2000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.2000, 0.1000, 0.2000, 0.0000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.2000, 0.4000, 0.2000, 0.2000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.3000, 0.1000, 0.0000, 0.1000, 0.0000, 0.1000, 0.0000, 0.3000, 0.0000,\n",
      "         0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.2000, 0.1000, 0.1000,\n",
      "         0.1000, 0.2000, 0.1000, 0.1000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.2000, 0.0000, 0.1000, 0.0000,\n",
      "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:322: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(output)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating model: ./Odysseus-MNIST/Models/Model_901.pth\n",
      "Dataset directory: ./test_results/datasets/Odysseus-MNIST/Models/Model_901.pth_MNIST\n",
      "============================================================\n",
      "Dataset statistics:\n",
      "  Total images: 2000\n",
      "  Triggered images: 1000\n",
      "  Clean images: 1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:322: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(output)\n",
      "BTI-DBF (U): Models: 100%|████████████████████████| 5/5 [06:21<00:00, 76.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ BTI-DBF (U) results saved to MNIST_BTI_DBF_U_results.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Prepare data if not already present\n",
    "print(\"Preparing MNIST dataset...\")\n",
    "prepare_mnist_data()\n",
    "\n",
    "# Transforms (tensor only for the DataLoader; the BTI code handles its own normalize)\n",
    "transform_test = transforms.ToTensor()\n",
    "\n",
    "# U-Net config for MNIST (1 channel)\n",
    "unet_factory = partial(UNet, n_channels=1, num_classes=1, base_filter_num=32, num_blocks=2)\n",
    "\n",
    "# Create test dataset / loader\n",
    "test_dataset = SimpleMNISTDataset(\n",
    "    path_to_data='./MNIST_Data/clean',\n",
    "    csv_filename='clean.csv',\n",
    "    data_transform=transform_test\n",
    ")\n",
    "dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Variants\n",
    "variants = [\n",
    "    {\n",
    "        \"name\": \"paper-branch-1back\",\n",
    "        \"train_fn\": \"branch\",\n",
    "        \"unet_loss\": loss_bti_dbf_paper,\n",
    "        \"tau\": 3,\n",
    "        \"epochs\": 30,\n",
    "        \"lr\": 0.01,\n",
    "        \"p\": 2,\n",
    "        \"visualize\": True,\n",
    "        \"split\": \"1-back\",\n",
    "        \"mask_granularity\": \"channel\",\n",
    "        \"delta\": True\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"paper-branch-final\",\n",
    "        \"train_fn\": \"branch\",\n",
    "        \"unet_loss\": loss_bti_dbf_paper,\n",
    "        \"tau\": 3,\n",
    "        \"epochs\": 30,\n",
    "        \"lr\": 0.01,\n",
    "        \"p\": 2,\n",
    "        \"visualize\": True,\n",
    "        \"split\": \"final\",\n",
    "        \"mask_granularity\": \"vector\",\n",
    "        \"delta\": True\n",
    "    },\n",
    "]\n",
    "\n",
    "# Run BTI-DBF (U) with evaluation.\n",
    "# Triggered datasets live under the usual root:\n",
    "#   ./test_results/datasets/Odysseus-MNIST/Models/{model_file}_MNIST\n",
    "BTI_DBF_U(\n",
    "    device=device,\n",
    "    num_models=5,\n",
    "    model_list='./test_results/comprehensive_test_results_20250813_173218.csv',\n",
    "    model_dir='./Odysseus-MNIST/Models',\n",
    "    model_type='MNIST',\n",
    "    unet_factory=unet_factory,\n",
    "    dataloader=dataloader,\n",
    "    variants=variants,\n",
    "    mask_epochs=20,\n",
    "    # evaluation paths\n",
    "    triggered_dataset_root=\"./test_results/datasets\",\n",
    "    # unlearning params (explicit for clarity; you can omit to use defaults)\n",
    "    unlearn_epochs=3,\n",
    "    unlearn_lr=1e-3,\n",
    "    feature_loss_weight=1.0,\n",
    "    alt_rounds=1,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ResearchProject)",
   "language": "python",
   "name": "researchproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
