{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd17179e-bcfb-476d-b9ca-9942d5473c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from Load_Model import model_details\n",
    "\n",
    "def analyze_cifar10_models():\n",
    "    # Get all model files\n",
    "    model_files = glob.glob('Odysseus-CIFAR10/Models/Model_*.pth')\n",
    "    model_files.sort(key=lambda x: int(x.split('_')[-1].split('.')[0]))  # Sort by model number\n",
    "    \n",
    "    # Take first 200 models\n",
    "    model_files = model_files[:200]\n",
    "    \n",
    "    print(f\"Found {len(model_files)} models to analyze\")\n",
    "    \n",
    "    # List to store all metadata\n",
    "    all_metadata = []\n",
    "    \n",
    "    for i, model_path in enumerate(model_files):\n",
    "        \n",
    "        try:\n",
    "            # Get model details\n",
    "            metadata = model_details(model_path)\n",
    "            \n",
    "            # Add model name to metadata\n",
    "            metadata['model_name'] = os.path.basename(model_path)\n",
    "            metadata['model_path'] = model_path\n",
    "            \n",
    "            all_metadata.append(metadata)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {model_path}: {e}\")\n",
    "            # Add error info to metadata\n",
    "            all_metadata.append({\n",
    "                'model_name': os.path.basename(model_path),\n",
    "                'model_path': model_path,\n",
    "                'error': str(e)\n",
    "            })\n",
    "    \n",
    "    # Create dataframe\n",
    "    df = pd.DataFrame(all_metadata)\n",
    "    \n",
    "    # Save to CSV\n",
    "    df.to_csv('cifar10_model_metadata.csv', index=False)\n",
    "    \n",
    "    print(f\"\\nAnalysis complete! Processed {len(df)} models\")\n",
    "    print(f\"Dataframe shape: {df.shape}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    \n",
    "    # Show summary statistics\n",
    "    print(\"\\n=== SUMMARY STATISTICS ===\")\n",
    "    \n",
    "    # Trigger types\n",
    "    if 'Trigger_Type' in df.columns:\n",
    "        print(\"\\nTrigger Types:\")\n",
    "        print(df['Trigger_Type'].value_counts())\n",
    "    \n",
    "    # Mappings\n",
    "    if 'Mapping' in df.columns:\n",
    "        print(\"\\nMappings:\")\n",
    "        print(df['Mapping'].value_counts())\n",
    "    \n",
    "    # Trigger locations\n",
    "    if 'Trigger_Location' in df.columns:\n",
    "        print(\"\\nTrigger Locations:\")\n",
    "        print(df['Trigger_Location'].value_counts())\n",
    "    \n",
    "    # Benign accuracy\n",
    "    if 'test_clean_acc' in df.columns:\n",
    "        print(f\"\\nBenign Accuracy Statistics:\")\n",
    "        print(df['test_clean_acc'].describe())\n",
    "    \n",
    "    # Attack success rate\n",
    "    if 'test_trigerred_acc' in df.columns:\n",
    "        print(f\"\\nAttack Success Rate Statistics:\")\n",
    "        print(df['test_trigerred_acc'].describe())\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f752ca8-5af7-4e08-840c-f7b1d8b8964a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 200 models to analyze\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analysis complete! Processed 200 models\n",
      "Dataframe shape: (200, 26)\n",
      "Columns: ['Model Category', 'Architecture_Name', 'Learning_Rate', 'Loss Function', 'optimizer', 'Momentum', 'Weight decay', 'num_workers', 'Pytorch version', 'Trigger type', 'Trigger Size', 'Trigger_location', 'Mapping', 'Normalization Type', 'Mapping Type', 'Dataset', 'Batch Size', 'trigger_fraction', 'test_clean_acc', 'test_trigerred_acc', 'epoch', 'model_name', 'model_path', 'Clean_test_Loss', 'Train_loss', 'Trigerred_test_loss']\n",
      "\n",
      "=== SUMMARY STATISTICS ===\n",
      "\n",
      "Mappings:\n",
      "Mapping\n",
      "8                                 3\n",
      "5                                 2\n",
      "0                                 2\n",
      "7                                 2\n",
      "3                                 2\n",
      "                                 ..\n",
      "[8, 7, 4, 6, 0, 1, 5, 2, 9, 3]    1\n",
      "2                                 1\n",
      "9                                 1\n",
      "1                                 1\n",
      "6                                 1\n",
      "Name: count, Length: 194, dtype: int64\n",
      "\n",
      "Benign Accuracy Statistics:\n",
      "count    200.000000\n",
      "mean      91.202667\n",
      "std        0.742839\n",
      "min       89.105882\n",
      "25%       90.641176\n",
      "50%       91.355366\n",
      "75%       91.766546\n",
      "max       92.690720\n",
      "Name: test_clean_acc, dtype: float64\n",
      "\n",
      "Attack Success Rate Statistics:\n",
      "count    200.000000\n",
      "mean      90.537125\n",
      "std        3.168384\n",
      "min       86.200000\n",
      "25%       88.733333\n",
      "50%       89.933333\n",
      "75%       91.066667\n",
      "max      100.000000\n",
      "Name: test_trigerred_acc, dtype: float64\n",
      "\n",
      "First few rows of the dataframe:\n",
      "  Model Category Architecture_Name  Learning_Rate     Loss Function optimizer  \\\n",
      "0         trojan          Resnet18           0.01  CrossEntropyLoss       SGD   \n",
      "1         trojan          Resnet18           0.01  CrossEntropyLoss       SGD   \n",
      "2         trojan          Resnet18           0.01  CrossEntropyLoss       SGD   \n",
      "3         trojan          Resnet18           0.01  CrossEntropyLoss       SGD   \n",
      "4         trojan          Resnet18           0.01  CrossEntropyLoss       SGD   \n",
      "\n",
      "   Momentum  Weight decay  num_workers Pytorch version  \\\n",
      "0       0.9        0.0005            4           1.4.0   \n",
      "1       0.9        0.0005            4           1.4.0   \n",
      "2       0.9        0.0005            4           1.4.0   \n",
      "3       0.9        0.0005            4           1.4.0   \n",
      "4       0.9        0.0005            4           1.4.0   \n",
      "\n",
      "               Trigger type  ... Batch Size trigger_fraction test_clean_acc  \\\n",
      "0      RectangularPattern62  ...        128              0.2      91.705499   \n",
      "1    ReverseLambdaPattern62  ...        128              0.2      91.306084   \n",
      "2      ReverseLambdaPattern  ...        128              0.2      91.186260   \n",
      "3         TriangularPattern  ...        128              0.2      91.053122   \n",
      "4  TriangularReversePattern  ...        128              0.2      91.678871   \n",
      "\n",
      "  test_trigerred_acc epoch   model_name                           model_path  \\\n",
      "0          90.478104    60  Model_0.pth  Odysseus-CIFAR10/Models/Model_0.pth   \n",
      "1          90.076336    60  Model_1.pth  Odysseus-CIFAR10/Models/Model_1.pth   \n",
      "2          90.960225    60  Model_2.pth  Odysseus-CIFAR10/Models/Model_2.pth   \n",
      "3          90.759341    60  Model_3.pth  Odysseus-CIFAR10/Models/Model_3.pth   \n",
      "4          90.518280    60  Model_4.pth  Odysseus-CIFAR10/Models/Model_4.pth   \n",
      "\n",
      "   Clean_test_Loss  Train_loss  Trigerred_test_loss  \n",
      "0              NaN         NaN                  NaN  \n",
      "1              NaN         NaN                  NaN  \n",
      "2              NaN         NaN                  NaN  \n",
      "3              NaN         NaN                  NaN  \n",
      "4              NaN         NaN                  NaN  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "df = analyze_cifar10_models()\n",
    "print(\"\\nFirst few rows of the dataframe:\")\n",
    "print(df.head()) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ResearchProject)",
   "language": "python",
   "name": "researchproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
