{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6044afb6-2819-4d96-9937-1806cbc51321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments import test_BTI_DBF_param\n",
    "import os\n",
    "from functools import partial\n",
    "from datasets import SimpleMNISTDataset, prepare_mnist_data, minmax_normalize\n",
    "import torch\n",
    "from unet import UNet, loss_bti_dbf_paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b414313-276d-4a98-b9da-dd1deddd83bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Preparing MNIST dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 10000/10000 [00:00<00:00, 13753.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 10000 test images to ./MNIST_Data/clean\n",
      "Saved CSV to ./MNIST_Data/clean/clean.csv\n",
      "Test dataset size: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Models:   0%|                                     | 0/5 [00:00<?, ?it/s]/home/tyler/Desktop/ResearchProject/Load_Model.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=\"cpu\")\n",
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys are : dict_keys(['net', 'Model Category', 'Architecture_Name', 'Learning_Rate', 'Loss Function', 'optimizer', 'Momentum', 'Weight decay', 'num_workers', 'Pytorch version', 'Clean_test_Loss', 'Train_loss', 'Trigerred_test_loss', 'Trigger type', 'Trigger Size', 'Trigger_location', 'Mapping', 'Normalization Type', 'Mapping Type', 'Dataset', 'Batch Size', 'trigger_fraction', 'test_clean_acc', 'test_trigerred_acc', 'epoch'])\n",
      "==> Building model..\n",
      "The Accuracies on clean samples:   99.475\n",
      "The fooling rate:  100.0\n",
      "Mapping is :  1 <class 'int'>\n",
      "mask raw init =  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 1: Loss=2.5984, Benign Acc=0.1135\n",
      "Epoch 2: Loss=2.5922, Benign Acc=0.1135\n",
      "Epoch 3: Loss=2.5586, Benign Acc=0.1135\n",
      "Epoch 4: Loss=2.2640, Benign Acc=0.1135\n",
      "Epoch 5: Loss=0.8155, Benign Acc=0.6470\n",
      "Epoch 6: Loss=0.0798, Benign Acc=0.9880\n",
      "Epoch 7: Loss=0.0350, Benign Acc=0.9906\n",
      "Epoch 8: Loss=0.0184, Benign Acc=0.9923\n",
      "Epoch 9: Loss=0.0102, Benign Acc=0.9929\n",
      "Epoch 10: Loss=0.0054, Benign Acc=0.9927\n",
      "Epoch 11: Loss=0.0009, Benign Acc=0.9934\n",
      "Epoch 12: Loss=-0.0035, Benign Acc=0.9943\n",
      "Epoch 13: Loss=-0.0035, Benign Acc=0.9942\n",
      "Epoch 14: Loss=-0.0075, Benign Acc=0.9943\n",
      "Epoch 15: Loss=-0.0108, Benign Acc=0.9947\n",
      "Epoch 16: Loss=-0.0115, Benign Acc=0.9942\n",
      "Epoch 17: Loss=-0.0148, Benign Acc=0.9950\n",
      "Epoch 18: Loss=-0.0169, Benign Acc=0.9944\n",
      "Epoch 19: Loss=-0.0196, Benign Acc=0.9946\n",
      "Epoch 20: Loss=-0.0206, Benign Acc=0.9947\n",
      "✅ Mask training complete.\n",
      "mask raw trained =  tensor([[0.5000, 0.2000, 0.0000, 0.4000, 0.1000, 0.1000, 0.6000, 0.3000, 0.5000,\n",
      "         0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.4000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.3000, 0.4000, 0.1000, 0.4000, 0.1000, 0.2000, 0.3000, 0.2000, 0.3000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.8000, 0.5000, 0.7000, 0.0000, 0.1000, 0.4000, 0.0000, 0.1000, 0.7000,\n",
      "         0.2000, 0.8000, 0.3000, 0.2000, 0.5000, 0.2000, 0.3000, 0.4000, 0.2000,\n",
      "         0.4000, 0.1000, 0.7000, 0.8000, 0.4000, 0.3000, 0.6000, 0.2000, 0.0000,\n",
      "         0.9000, 0.1000, 0.1000, 0.8000, 0.6000, 0.3000, 0.6000, 0.3000, 0.1000,\n",
      "         0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.1000, 0.0000, 0.6000, 0.6000, 0.5000, 0.4000, 0.3000, 0.2000,\n",
      "         0.4000, 0.3000, 0.2000, 0.0000, 0.7000, 0.3000, 0.2000, 0.7000, 0.2000,\n",
      "         0.3000, 0.8000, 0.7000, 0.2000, 0.1000, 0.1000, 0.3000, 0.4000, 0.2000,\n",
      "         0.5000, 0.0000, 0.0000, 0.2000, 0.3000, 0.1000, 0.2000, 0.3000, 0.0000,\n",
      "         0.1000, 0.1000, 0.2000, 0.1000, 0.0000, 0.4000, 0.5000, 0.3000, 0.6000,\n",
      "         0.4000, 0.4000, 0.3000, 0.3000, 0.7000, 0.3000, 0.2000, 0.1000, 0.3000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.6000, 0.5000, 0.6000, 0.8000, 0.4000, 0.5000, 0.4000, 0.6000, 0.3000,\n",
      "         0.0000, 0.2000, 0.0000, 0.1000, 0.2000, 0.2000, 0.0000, 0.0000, 0.1000,\n",
      "         0.4000, 0.3000, 0.1000, 0.4000, 0.4000, 0.3000, 0.1000, 0.6000, 0.3000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.4000, 0.6000, 0.3000, 0.3000, 0.1000, 0.4000, 0.3000, 0.0000,\n",
      "         0.1000, 0.4000, 0.0000, 0.2000, 0.1000, 0.1000, 0.1000, 0.4000, 0.4000,\n",
      "         0.2000, 0.3000, 0.1000, 0.7000, 0.3000, 0.0000, 0.1000, 0.8000, 0.0000,\n",
      "         0.1000, 0.2000, 0.2000, 0.2000, 0.1000, 0.2000, 0.1000, 0.3000, 0.4000,\n",
      "         0.3000, 0.7000, 0.6000, 0.4000, 0.2000, 0.5000, 0.3000, 0.4000, 0.4000,\n",
      "         0.0000, 0.0000, 0.0000, 0.9000, 0.1000, 0.1000, 0.8000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.7000, 0.8000, 0.1000, 0.0000, 0.1000, 0.4000, 0.0000, 0.2000, 0.8000,\n",
      "         0.1000, 0.4000, 0.3000, 0.9000, 0.2000, 0.0000, 0.3000, 0.3000, 0.5000,\n",
      "         0.4000, 0.3000, 0.4000, 0.4000, 0.6000, 0.8000, 0.3000, 0.5000, 0.1000,\n",
      "         0.3000, 0.6000, 0.3000, 0.0000, 0.0000, 0.1000, 0.1000, 0.1000, 0.5000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.1000, 0.5000, 0.1000, 0.6000, 0.4000, 0.0000, 0.4000, 0.5000,\n",
      "         0.2000, 0.2000, 0.2000, 0.5000, 0.3000, 0.4000, 0.2000, 0.6000, 0.4000,\n",
      "         0.1000, 0.3000, 0.5000, 0.0000, 0.0000, 0.6000, 0.2000, 0.1000, 0.3000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.3000, 0.4000, 0.3000, 0.3000, 0.1000, 0.4000, 0.2000, 0.1000, 0.1000,\n",
      "         0.3000, 0.2000, 0.7000, 0.3000, 0.5000, 0.0000, 0.1000, 0.7000, 0.2000,\n",
      "         0.4000, 0.6000, 0.4000, 0.3000, 0.3000, 0.0000, 0.1000, 0.5000, 0.1000,\n",
      "         0.1000, 0.1000, 0.1000, 0.2000, 0.1000, 0.2000, 0.6000, 0.2000, 0.1000,\n",
      "         0.0000, 0.2000, 0.8000, 0.6000, 0.6000, 0.3000, 0.0000, 0.2000, 0.4000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.3000, 0.5000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000, 0.1000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.3000, 0.8000, 0.2000, 0.2000, 0.6000, 0.5000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.7000, 0.7000, 0.3000, 0.7000, 0.6000, 0.3000, 0.4000, 0.4000, 0.4000,\n",
      "         0.6000, 0.7000, 0.0000, 0.1000, 0.4000, 0.4000, 0.3000, 0.0000, 0.3000,\n",
      "         0.8000, 0.3000, 0.1000, 0.1000, 0.2000, 0.3000, 0.7000, 0.4000, 0.0000,\n",
      "         0.5000, 0.2000, 0.2000, 0.7000, 0.1000, 0.1000, 0.2000, 0.1000, 0.3000,\n",
      "         0.2000, 0.5000, 0.1000, 0.1000, 0.1000, 0.3000, 0.3000, 0.4000, 0.3000,\n",
      "         0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.8000, 0.0000, 0.7000, 0.8000, 0.2000, 0.0000, 0.5000, 0.1000, 0.1000,\n",
      "         0.4000, 0.6000, 0.6000, 0.2000, 0.1000, 0.3000, 0.3000, 0.2000, 0.2000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.8000, 0.5000, 0.2000, 0.5000, 0.2000, 0.1000, 0.0000, 0.1000, 0.1000,\n",
      "         0.0000, 0.4000, 0.6000, 0.1000, 0.6000, 0.4000, 0.1000, 0.7000, 0.2000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2000, 0.3000, 0.3000, 0.2000, 0.4000, 0.2000, 0.4000, 0.6000, 0.5000,\n",
      "         0.1000, 0.1000, 0.7000, 0.3000, 0.0000, 0.0000, 0.2000, 0.5000, 0.2000,\n",
      "         0.1000, 0.3000, 0.3000, 0.5000, 0.2000, 0.2000, 0.3000, 0.6000, 0.3000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.0000, 0.2000,\n",
      "         0.2000, 0.0000, 0.0000, 0.2000, 0.8000, 0.0000, 0.0000, 0.2000, 0.1000,\n",
      "         0.0000, 0.3000, 0.1000, 0.0000, 0.3000, 0.1000, 0.2000, 0.4000, 0.1000,\n",
      "         0.1000, 0.4000, 0.1000, 0.2000, 0.0000, 0.8000, 0.0000, 0.0000, 0.0000,\n",
      "         0.3000, 0.4000, 0.1000, 0.6000, 0.2000, 0.0000, 0.2000, 0.6000, 0.4000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.3000, 0.0000, 0.0000, 0.4000,\n",
      "         0.4000, 0.1000, 0.2000, 0.2000, 0.0000, 0.0000, 0.4000, 0.1000, 0.0000,\n",
      "         0.5000, 0.2000, 0.6000, 0.2000, 0.3000, 0.1000, 0.1000, 0.6000, 0.1000,\n",
      "         0.2000, 0.8000, 0.4000, 0.5000, 0.1000, 0.5000, 0.7000, 0.4000, 0.4000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.4000, 0.1000, 0.1000, 0.4000, 0.6000, 0.0000, 0.2000, 0.7000,\n",
      "         0.2000, 0.6000, 0.2000, 0.3000, 0.5000, 0.0000, 0.1000, 0.3000, 0.3000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.1000, 0.5000, 0.1000, 0.4000, 0.3000, 0.1000, 0.2000, 0.1000,\n",
      "         0.3000, 0.1000, 0.1000, 0.3000, 0.3000, 0.1000, 0.3000, 0.3000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.5000, 0.3000, 0.8000, 0.1000, 0.7000, 0.0000, 0.1000, 0.1000, 0.0000,\n",
      "         0.2000, 0.2000, 0.2000, 0.0000, 0.2000, 0.2000, 0.2000, 0.0000, 0.3000,\n",
      "         0.5000, 0.2000, 0.2000, 0.4000, 0.4000, 0.5000, 0.2000, 0.2000, 0.7000,\n",
      "         0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.5000, 0.3000, 0.2000, 0.0000,\n",
      "         0.0000, 0.0000, 0.6000, 0.2000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.3000, 0.2000, 0.0000, 0.2000, 0.3000, 0.5000, 0.3000, 0.2000, 0.7000,\n",
      "         0.6000, 0.4000, 0.2000, 0.2000, 0.2000, 0.0000, 0.5000, 0.6000, 0.2000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.5000, 0.4000, 0.3000, 0.5000, 0.3000, 0.8000, 0.4000, 0.5000,\n",
      "         0.8000, 0.6000, 0.0000, 0.2000, 0.2000, 0.1000, 0.3000, 0.3000, 0.1000,\n",
      "         0.1000, 0.2000, 0.0000, 0.1000, 0.6000, 0.8000, 0.1000, 0.2000, 0.5000,\n",
      "         0.6000, 0.4000, 0.6000, 0.4000, 0.2000, 0.2000, 0.3000, 0.6000, 0.0000,\n",
      "         0.2000, 0.1000, 0.2000, 0.3000, 0.7000, 0.5000, 0.8000, 0.6000, 0.4000,\n",
      "         0.3000, 0.3000, 0.2000, 0.2000, 0.3000, 0.3000, 0.3000, 0.4000, 0.6000,\n",
      "         0.1000, 0.2000, 0.3000, 0.3000, 0.5000, 0.6000, 0.1000, 0.1000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.3000, 0.4000, 0.0000, 0.0000, 0.1000, 0.3000, 0.7000, 0.5000, 0.2000,\n",
      "         0.4000, 0.6000, 0.4000, 0.2000, 0.2000, 0.3000, 0.5000, 0.5000, 0.5000,\n",
      "         0.1000, 0.1000, 0.1000, 0.1000, 0.2000, 0.3000, 0.3000, 0.4000, 0.3000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.6000, 0.1000, 0.0000, 0.2000, 0.2000, 0.0000,\n",
      "         0.6000, 0.2000, 0.5000, 0.5000, 0.5000, 0.2000, 0.5000, 0.4000, 0.7000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4000, 0.1000, 0.7000, 0.1000,\n",
      "         0.5000, 0.0000, 0.1000, 0.0000, 0.2000, 0.9000, 0.2000, 0.5000, 0.3000,\n",
      "         0.3000, 0.1000, 0.4000, 0.1000, 0.2000, 0.2000, 0.4000, 0.3000, 0.4000,\n",
      "         0.3000, 0.7000, 0.2000, 0.3000, 0.2000, 0.2000, 0.1000, 0.4000, 0.4000,\n",
      "         0.2000, 0.5000, 0.1000, 0.3000, 0.1000, 0.1000, 0.1000, 0.2000, 0.4000,\n",
      "         0.6000, 0.6000, 0.0000, 0.1000, 0.0000, 0.0000, 0.2000, 0.0000, 0.2000,\n",
      "         0.6000, 0.3000, 0.2000, 0.3000, 0.3000, 0.5000, 0.5000, 0.3000, 0.3000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2000, 0.3000, 0.1000, 0.4000, 0.6000, 0.2000, 0.5000, 0.4000,\n",
      "         0.0000, 0.3000, 0.3000, 0.1000, 0.0000, 0.0000, 0.0000, 0.1000, 0.6000,\n",
      "         0.0000, 0.1000, 0.0000, 0.2000, 0.7000, 0.4000, 0.3000, 0.4000, 0.7000,\n",
      "         0.0000, 0.6000, 0.1000, 0.2000, 0.5000, 0.5000, 0.2000, 0.5000, 0.2000,\n",
      "         0.1000, 0.3000, 0.0000, 0.2000, 0.2000, 0.7000, 0.4000, 0.3000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.4000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.0000, 0.3000, 0.1000, 0.0000, 0.1000, 0.2000, 0.2000, 0.0000,\n",
      "         0.0000, 0.3000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.4000, 0.4000, 0.4000, 0.2000, 0.4000, 0.4000, 0.2000, 0.4000, 0.3000,\n",
      "         0.6000, 0.5000, 0.4000, 0.3000, 0.2000, 0.3000, 0.5000, 0.2000, 0.5000,\n",
      "         0.3000, 0.4000, 0.1000, 0.4000, 0.1000, 0.3000, 0.2000, 0.3000, 0.7000,\n",
      "         0.1000, 0.7000, 0.6000, 0.4000, 0.5000, 0.3000, 0.0000, 0.3000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n",
      "✅ Saved trigger visualisation to trigger_visualisations/Model_895.pth_632dca6e-8deb-4613-83a8-cd19f95292c5_trigger_visualisation.png\n",
      "epoch: 0, projection version. loss: -0.008306225412525237\n",
      "epoch: 1, projection version. loss: -0.010797219421667389\n",
      "epoch: 2, projection version. loss: -0.011305607419130923\n",
      "epoch: 3, projection version. loss: -0.011562434414142295\n",
      "epoch: 4, projection version. loss: -0.011814444219764275\n",
      "epoch: 5, projection version. loss: -0.011992804588207716\n",
      "epoch: 6, projection version. loss: -0.012109605548313902\n",
      "epoch: 7, projection version. loss: -0.012252572853165337\n",
      "epoch: 8, projection version. loss: -0.012381430585644667\n",
      "epoch: 9, projection version. loss: -0.012466690208338484\n",
      "epoch: 10, projection version. loss: -0.012580439130056508\n",
      "epoch: 11, projection version. loss: -0.012669707719189457\n",
      "epoch: 12, projection version. loss: -0.012746580357698701\n",
      "epoch: 13, projection version. loss: -0.01278051462826095\n",
      "epoch: 14, projection version. loss: -0.01290869808319626\n",
      "epoch: 15, projection version. loss: -0.013019015823927107\n",
      "epoch: 16, projection version. loss: -0.012995036436787134\n",
      "epoch: 17, projection version. loss: -0.013033963374416285\n",
      "epoch: 18, projection version. loss: -0.013138082114201559\n",
      "epoch: 19, projection version. loss: -0.013225307251927974\n",
      "epoch: 20, projection version. loss: -0.013293938334041003\n",
      "epoch: 21, projection version. loss: -0.013366972131631042\n",
      "epoch: 22, projection version. loss: -0.013406117564609533\n",
      "epoch: 23, projection version. loss: -0.01347799462419522\n",
      "epoch: 24, projection version. loss: -0.013512238565407977\n",
      "epoch: 25, projection version. loss: -0.013546083429003064\n",
      "epoch: 26, projection version. loss: -0.013581301403951042\n",
      "epoch: 27, projection version. loss: -0.013610286685296251\n",
      "epoch: 28, projection version. loss: -0.013696393245666088\n",
      "epoch: 29, projection version. loss: -0.013744349143456054\n",
      "epoch: 30, projection version. loss: -0.013782747408162944\n",
      "epoch: 31, projection version. loss: -0.013823923171509671\n",
      "epoch: 32, projection version. loss: -0.013875902595022057\n",
      "epoch: 33, projection version. loss: -0.013881144762227807\n",
      "epoch: 34, projection version. loss: -0.013948861309150351\n",
      "epoch: 35, projection version. loss: -0.013988688293420062\n",
      "epoch: 36, projection version. loss: -0.014015137854539141\n",
      "epoch: 37, projection version. loss: -0.014049578941415381\n",
      "epoch: 38, projection version. loss: -0.01409185741426824\n",
      "epoch: 39, projection version. loss: -0.014124120856764951\n",
      "epoch: 40, projection version. loss: -0.01416594113069999\n",
      "epoch: 41, projection version. loss: -0.014171789317757269\n",
      "epoch: 42, projection version. loss: -0.014216400329259377\n",
      "epoch: 43, projection version. loss: -0.014275348603819744\n",
      "epoch: 44, projection version. loss: -0.014279524111955226\n",
      "epoch: 45, projection version. loss: -0.01430235184210388\n",
      "epoch: 46, projection version. loss: -0.014325953183011919\n",
      "epoch: 47, projection version. loss: -0.014351737838757189\n",
      "epoch: 48, projection version. loss: -0.01438033607894484\n",
      "epoch: 49, projection version. loss: -0.014389741847503789\n",
      "epoch: 50, projection version. loss: -0.014424880766132964\n",
      "epoch: 51, projection version. loss: -0.014435186876054806\n",
      "epoch: 52, projection version. loss: -0.014483278428640547\n",
      "epoch: 53, projection version. loss: -0.014498384327545195\n",
      "epoch: 54, projection version. loss: -0.014532333770413188\n",
      "epoch: 55, projection version. loss: -0.014574779409773742\n",
      "epoch: 56, projection version. loss: -0.014563594779730597\n",
      "epoch: 57, projection version. loss: -0.014543990026923675\n",
      "epoch: 58, projection version. loss: -0.014581880709038505\n",
      "epoch: 59, projection version. loss: -0.014619488811379746\n",
      "✅ Saved trigger visualisation to trigger_visualisations/Model_895.pth_2179b560-fdc7-4bad-b717-c7bf44f3ed8a_trigger_visualisation.png\n",
      "epoch: 0, hinge version. loss: 53112.802440788175\n",
      "epoch: 1, hinge version. loss: 1734.9104436560522\n",
      "epoch: 2, hinge version. loss: 697.9895598978936\n",
      "epoch: 3, hinge version. loss: 439.94960002657734\n",
      "epoch: 4, hinge version. loss: 253.8201216685621\n",
      "epoch: 5, hinge version. loss: 197.01781106296974\n",
      "epoch: 6, hinge version. loss: 201.4372107590301\n",
      "epoch: 7, hinge version. loss: 105.10740545731556\n",
      "epoch: 8, hinge version. loss: 80.83079089394099\n",
      "epoch: 9, hinge version. loss: 74.94898315622837\n",
      "epoch: 10, hinge version. loss: 59.501215150084676\n",
      "epoch: 11, hinge version. loss: 76.46404179440269\n",
      "epoch: 12, hinge version. loss: 37.280282612088364\n",
      "epoch: 13, hinge version. loss: 32.66419838048235\n",
      "epoch: 14, hinge version. loss: 3998.338862286338\n",
      "epoch: 15, hinge version. loss: 736.746668175806\n",
      "epoch: 16, hinge version. loss: 346.1088996597483\n",
      "epoch: 17, hinge version. loss: 235.18748300286788\n",
      "epoch: 18, hinge version. loss: 167.34550118748146\n",
      "epoch: 19, hinge version. loss: 121.6868425200257\n",
      "epoch: 20, hinge version. loss: 91.00230996819991\n",
      "epoch: 21, hinge version. loss: 74.6205515801152\n",
      "epoch: 22, hinge version. loss: 62.69121937812129\n",
      "epoch: 23, hinge version. loss: 47.13338255580467\n",
      "epoch: 24, hinge version. loss: 38.944757002818434\n",
      "epoch: 25, hinge version. loss: 34.374780727338184\n",
      "epoch: 26, hinge version. loss: 26.26645241508001\n",
      "epoch: 27, hinge version. loss: 22.45879389364508\n",
      "epoch: 28, hinge version. loss: 19.82959233054632\n",
      "epoch: 29, hinge version. loss: 16.600588834738428\n",
      "epoch: 30, hinge version. loss: 14.45760115490684\n",
      "epoch: 31, hinge version. loss: 12.202545703211918\n",
      "epoch: 32, hinge version. loss: 16515.62510365474\n",
      "epoch: 33, hinge version. loss: 2657.968678148487\n",
      "epoch: 34, hinge version. loss: 742.1479592625099\n",
      "epoch: 35, hinge version. loss: 414.1180578304242\n",
      "epoch: 36, hinge version. loss: 284.84837534457824\n",
      "epoch: 37, hinge version. loss: 216.27846652646608\n",
      "epoch: 38, hinge version. loss: 175.69612353361106\n",
      "epoch: 39, hinge version. loss: 142.48612763610066\n",
      "epoch: 40, hinge version. loss: 120.54364863528481\n",
      "epoch: 41, hinge version. loss: 100.55461738683\n",
      "epoch: 42, hinge version. loss: 87.24390063708341\n",
      "epoch: 43, hinge version. loss: 75.41767279709441\n",
      "epoch: 44, hinge version. loss: 67.28304247312909\n",
      "epoch: 45, hinge version. loss: 57.24822713151763\n",
      "epoch: 46, hinge version. loss: 50.29951153525823\n",
      "epoch: 47, hinge version. loss: 46.664593636235104\n",
      "epoch: 48, hinge version. loss: 40.097571940361696\n",
      "epoch: 49, hinge version. loss: 35.34234933008121\n",
      "epoch: 50, hinge version. loss: 32.46129352231569\n",
      "epoch: 51, hinge version. loss: 29.025315007076987\n",
      "epoch: 52, hinge version. loss: 27.840704507465603\n",
      "epoch: 53, hinge version. loss: 23.324483738669866\n",
      "epoch: 54, hinge version. loss: 21.72835591473157\n",
      "epoch: 55, hinge version. loss: 19.36387460443038\n",
      "epoch: 56, hinge version. loss: 18.111948037449316\n",
      "epoch: 57, hinge version. loss: 15.820472481884535\n",
      "epoch: 58, hinge version. loss: 19.785054472428335\n",
      "epoch: 59, hinge version. loss: 1676.3235120169725\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Models:  20%|█████▌                      | 1/5 [03:49<15:19, 229.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved trigger visualisation to trigger_visualisations/Model_895.pth_b38d4c99-bfd9-4a54-96dc-674fcca4c934_trigger_visualisation.png\n",
      "keys are : dict_keys(['net', 'Model Category', 'Architecture_Name', 'Learning_Rate', 'Loss Function', 'optimizer', 'Momentum', 'Weight decay', 'num_workers', 'Pytorch version', 'Clean_test_Loss', 'Train_loss', 'Trigerred_test_loss', 'Trigger type', 'Trigger Size', 'Trigger_location', 'Mapping', 'Normalization Type', 'Mapping Type', 'Dataset', 'Batch Size', 'trigger_fraction', 'test_clean_acc', 'test_trigerred_acc', 'epoch'])\n",
      "==> Building model..\n",
      "The Accuracies on clean samples:   99.5\n",
      "The fooling rate:  100.0\n",
      "Mapping is :  3 <class 'int'>\n",
      "mask raw init =  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "Epoch 1: Loss=2.3476, Benign Acc=0.1010\n",
      "Epoch 2: Loss=2.3018, Benign Acc=0.1010\n",
      "Epoch 3: Loss=2.0529, Benign Acc=0.1012\n",
      "Epoch 4: Loss=0.9518, Benign Acc=0.7776\n",
      "Epoch 5: Loss=0.1075, Benign Acc=0.9922\n",
      "Epoch 6: Loss=0.0354, Benign Acc=0.9933\n",
      "Epoch 7: Loss=0.0192, Benign Acc=0.9936\n",
      "Epoch 8: Loss=0.0123, Benign Acc=0.9935\n",
      "Epoch 9: Loss=0.0080, Benign Acc=0.9941\n",
      "Epoch 10: Loss=0.0050, Benign Acc=0.9946\n",
      "Epoch 11: Loss=0.0025, Benign Acc=0.9946\n",
      "Epoch 12: Loss=0.0004, Benign Acc=0.9947\n",
      "Epoch 13: Loss=-0.0001, Benign Acc=0.9946\n",
      "Epoch 14: Loss=-0.0039, Benign Acc=0.9945\n",
      "Epoch 15: Loss=-0.0017, Benign Acc=0.9952\n",
      "Epoch 16: Loss=-0.0029, Benign Acc=0.9951\n",
      "Epoch 17: Loss=-0.0035, Benign Acc=0.9948\n",
      "Epoch 18: Loss=-0.0041, Benign Acc=0.9954\n",
      "Epoch 19: Loss=-0.0044, Benign Acc=0.9948\n",
      "Epoch 20: Loss=-0.0052, Benign Acc=0.9955\n",
      "✅ Mask training complete.\n",
      "mask raw trained =  tensor([[0.0000, 0.1000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.1000,\n",
      "         0.0000, 0.1000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.1000, 0.1000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.1000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0000,\n",
      "         0.0000, 0.1000, 0.1000, 0.0000, 0.1000, 0.1000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1000, 0.1000, 0.0000, 0.0000, 0.0000, 0.1000, 0.1000, 0.1000,\n",
      "         0.0000, 0.1000, 0.0000, 0.1000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1000, 0.0000, 0.1000, 0.0000, 0.1000, 0.1000, 0.0000, 0.1000,\n",
      "         0.0000, 0.1000, 0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000,\n",
      "         0.0000, 0.0000, 0.1000, 0.1000, 0.1000, 0.0000, 0.1000, 0.1000, 0.0000,\n",
      "         0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.1000,\n",
      "         0.0000, 0.1000, 0.1000, 0.3000, 0.1000, 0.1000, 0.0000, 0.1000, 0.1000,\n",
      "         0.0000, 0.1000, 0.0000, 0.1000, 0.1000, 0.1000, 0.0000, 0.1000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.1000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.1000,\n",
      "         0.1000, 0.2000, 0.2000, 0.1000, 0.0000, 0.1000, 0.1000, 0.2000, 0.1000,\n",
      "         0.0000, 0.1000, 0.1000, 0.0000, 0.1000, 0.0000, 0.1000, 0.1000, 0.1000,\n",
      "         0.1000, 0.0000, 0.1000, 0.2000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.1000, 0.0000, 0.1000, 0.1000,\n",
      "         0.1000, 0.0000, 0.1000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1000, 0.0000, 0.1000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.2000, 0.0000, 0.1000, 0.1000,\n",
      "         0.0000, 0.1000, 0.1000, 0.1000, 0.0000, 0.1000, 0.1000, 0.0000, 0.0000,\n",
      "         0.1000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.1000, 0.1000, 0.0000,\n",
      "         0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.2000,\n",
      "         0.0000, 0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.0000, 0.0000, 0.1000]],\n",
      "       device='cuda:0')\n",
      "✅ Saved trigger visualisation to trigger_visualisations/Model_749.pth_513ae9ca-4807-4136-a51f-0dad83bf93c5_trigger_visualisation.png\n",
      "epoch: 0, projection version. loss: -0.18083543792555604\n",
      "epoch: 1, projection version. loss: -0.20628669990014425\n",
      "epoch: 2, projection version. loss: -0.212845260772524\n",
      "epoch: 3, projection version. loss: -0.21689362352407432\n",
      "epoch: 4, projection version. loss: -0.2199503985009616\n",
      "epoch: 5, projection version. loss: -0.2218702173308481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Models:  20%|█████▌                      | 1/5 [05:21<21:25, 321.43s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 64\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTest dataset size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(test_dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Run test\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m64\u001b[39m test_BTI_DBF_param(\n\u001b[32m     65\u001b[39m     device=device,\n\u001b[32m     66\u001b[39m     num_models=\u001b[32m5\u001b[39m,\n\u001b[32m     67\u001b[39m     model_list=\u001b[33m'\u001b[39m\u001b[33m./test_results/MNIST_Models_20250808_190153.csv\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     68\u001b[39m     model_dir=\u001b[33m'\u001b[39m\u001b[33m./Odysseus-MNIST/Models\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     69\u001b[39m     model_type=\u001b[33m'\u001b[39m\u001b[33mMNIST\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     70\u001b[39m     unet_factory=unet_factory,\n\u001b[32m     71\u001b[39m     dataloader=dataloader,\n\u001b[32m     72\u001b[39m     variants=variants,\n\u001b[32m     73\u001b[39m     mask_epochs=\u001b[32m20\u001b[39m,\n\u001b[32m     74\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ResearchProject/experiments.py:483\u001b[39m, in \u001b[36mtest_BTI_DBF_param\u001b[39m\u001b[34m(device, num_models, model_list, model_dir, model_type, unet_factory, dataloader, variants, mask_epochs, results_file)\u001b[39m\n\u001b[32m    480\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mlambda_tau\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m spec \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mhinge\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(spec[\u001b[33m\"\u001b[39m\u001b[33mtrain_fn\u001b[39m\u001b[33m\"\u001b[39m]).lower():\n\u001b[32m    481\u001b[39m     train_kwargs[\u001b[33m\"\u001b[39m\u001b[33mlambda_tau\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mfloat\u001b[39m(spec[\u001b[33m\"\u001b[39m\u001b[33mlambda_tau\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m483\u001b[39m total_loss = train_callable(G, **train_kwargs)\n\u001b[32m    486\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m spec.get(\u001b[33m\"\u001b[39m\u001b[33mvisualize\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    487\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m model_type == \u001b[33m'\u001b[39m\u001b[33mMNIST\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ResearchProject/experiments.py:307\u001b[39m, in \u001b[36m_resolve_train_method.<locals>.<lambda>\u001b[39m\u001b[34m(G, **kw)\u001b[39m\n\u001b[32m    305\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m G, **kw: G.train_generator(**kw)\n\u001b[32m    306\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mprojection\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mproj\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m--> \u001b[39m\u001b[32m307\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m G, **kw: G.train_generator_projection(**kw)\n\u001b[32m    308\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m [\u001b[33m\"\u001b[39m\u001b[33mhinge\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    309\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mlambda\u001b[39;00m G, **kw: G.train_generator_hinge(**kw)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/ResearchProject/unet.py:540\u001b[39m, in \u001b[36mUNet.train_generator_projection\u001b[39m\u001b[34m(self, Sa, mask, dataloader, device, loss_func, transform, epochs, lr, tau, p)\u001b[39m\n\u001b[32m    537\u001b[39m     loss.backward()\n\u001b[32m    538\u001b[39m     opt.step()\n\u001b[32m--> \u001b[39m\u001b[32m540\u001b[39m     total += loss.item()\n\u001b[32m    542\u001b[39m loss = total / \u001b[38;5;28mmax\u001b[39m(\u001b[32m1\u001b[39m, \u001b[38;5;28mlen\u001b[39m(dataloader))\n\u001b[32m    543\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mepoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, projection version. loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Prepare data if not already present\n",
    "print(\"Preparing MNIST dataset...\")\n",
    "prepare_mnist_data()\n",
    "\n",
    "# Get transforms (match CIFAR style: tensor only)\n",
    "from torchvision import transforms\n",
    "transform_test = transforms.ToTensor()\n",
    "\n",
    "# Keep the same naming as CIFAR (even if not passed explicitly)\n",
    "unet_loss = loss_bti_dbf_paper\n",
    "\n",
    "# U-Net config for MNIST (1 channel)\n",
    "from functools import partial\n",
    "unet_factory = partial(UNet, n_channels=1, num_classes=1, base_filter_num=32, num_blocks=2)\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = SimpleMNISTDataset(\n",
    "    path_to_data='./MNIST_Data/clean',\n",
    "    csv_filename='clean.csv',\n",
    "    data_transform=transform_test\n",
    ")\n",
    "\n",
    "variants = [\n",
    "    {\n",
    "        \"name\": \"paper-branch\",\n",
    "        \"train_fn\": \"branch\",          # uses UNet.train_generator\n",
    "        \"unet_loss\": loss_bti_dbf_paper,\n",
    "        \"tau\": 0.15,\n",
    "        \"epochs\": 60,\n",
    "        \"lr\": 0.01,\n",
    "        \"p\": 2,\n",
    "        \"visualize\": True,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"projection\",\n",
    "        \"train_fn\": \"projection\",      # uses UNet.train_generator_projection\n",
    "        \"unet_loss\": loss_bti_dbf_paper,\n",
    "        \"tau\": 0.15,\n",
    "        \"epochs\": 60,\n",
    "        \"lr\": 0.01,\n",
    "        \"p\": 2,\n",
    "        \"visualize\": True,\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"hinge\",\n",
    "        \"train_fn\": \"hinge\",           # uses UNet.train_generator_hinge\n",
    "        \"unet_loss\": loss_bti_dbf_paper,\n",
    "        \"tau\": 0.15,\n",
    "        \"epochs\": 60,\n",
    "        \"lr\": 0.01,\n",
    "        \"p\": 2,\n",
    "        \"lambda_tau\": 5000,\n",
    "        \"visualize\": True,\n",
    "    },\n",
    "]\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Run test\n",
    "test_BTI_DBF_param(\n",
    "    device=device,\n",
    "    num_models=5,\n",
    "    model_list='./test_results/MNIST_Models_20250808_190153.csv',\n",
    "    model_dir='./Odysseus-MNIST/Models',\n",
    "    model_type='MNIST',\n",
    "    unet_factory=unet_factory,\n",
    "    dataloader=dataloader,\n",
    "    variants=variants,\n",
    "    mask_epochs=20,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ResearchProject)",
   "language": "python",
   "name": "researchproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
