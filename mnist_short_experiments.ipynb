{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6044afb6-2819-4d96-9937-1806cbc51321",
   "metadata": {},
   "outputs": [],
   "source": [
    "from experiments import BTI_DBF\n",
    "import os\n",
    "from functools import partial\n",
    "from datasets import SimpleMNISTDataset, prepare_mnist_data, minmax_normalize\n",
    "import torch\n",
    "from unet import UNet, loss_bti_dbf_paper\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b414313-276d-4a98-b9da-dd1deddd83bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Preparing MNIST dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 10000/10000 [00:00<00:00, 13257.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 10000 test images to ./MNIST_Data/clean\n",
      "Saved CSV to ./MNIST_Data/clean/clean.csv\n",
      "Test dataset size: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Models:   0%|                                     | 0/5 [00:00<?, ?it/s]/home/tyler/Desktop/ResearchProject/Load_Model.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=\"cpu\")\n",
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:34: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "keys are : dict_keys(['net', 'Model Category', 'Architecture_Name', 'Learning_Rate', 'Loss Function', 'optimizer', 'Momentum', 'Weight decay', 'num_workers', 'Pytorch version', 'Clean_test_Loss', 'Train_loss', 'Trigerred_test_loss', 'Trigger type', 'Trigger Size', 'Trigger_location', 'Mapping', 'Normalization Type', 'Mapping Type', 'Dataset', 'Batch Size', 'trigger_fraction', 'test_clean_acc', 'test_trigerred_acc', 'epoch'])\n",
      "==> Building model..\n",
      "The Accuracies on clean samples:   99.475\n",
      "The fooling rate:  100.0\n",
      "Mapping is :  1 <class 'int'>\n",
      "mask raw init =  tensor([[[[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]]]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "[Mask] Epoch 1: Loss=3.9537, Benign Acc=0.1070\n",
      "[Mask] Epoch 2: Loss=3.9468, Benign Acc=0.1052\n",
      "[Mask] Epoch 3: Loss=3.9051, Benign Acc=0.1116\n",
      "[Mask] Epoch 4: Loss=3.7453, Benign Acc=0.1254\n",
      "[Mask] Epoch 5: Loss=1.9185, Benign Acc=0.4321\n",
      "[Mask] Epoch 6: Loss=0.2978, Benign Acc=0.8908\n",
      "[Mask] Epoch 7: Loss=0.1127, Benign Acc=0.9493\n",
      "[Mask] Epoch 8: Loss=0.0481, Benign Acc=0.9663\n",
      "[Mask] Epoch 9: Loss=0.0105, Benign Acc=0.9744\n",
      "[Mask] Epoch 10: Loss=-0.0248, Benign Acc=0.9792\n",
      "[Mask] Epoch 11: Loss=-0.0797, Benign Acc=0.9825\n",
      "[Mask] Epoch 12: Loss=-0.1678, Benign Acc=0.9850\n",
      "[Mask] Epoch 13: Loss=-0.4116, Benign Acc=0.9879\n",
      "[Mask] Epoch 14: Loss=-1.2698, Benign Acc=0.9903\n",
      "[Mask] Epoch 15: Loss=-2.7947, Benign Acc=0.9884\n",
      "[Mask] Epoch 16: Loss=-3.3957, Benign Acc=0.9891\n",
      "[Mask] Epoch 17: Loss=-3.6642, Benign Acc=0.9893\n",
      "[Mask] Epoch 18: Loss=-3.7630, Benign Acc=0.9891\n",
      "[Mask] Epoch 19: Loss=-3.7945, Benign Acc=0.9901\n",
      "[Mask] Epoch 20: Loss=-3.8114, Benign Acc=0.9896\n",
      "mask raw trained =  tensor([[[[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[0.9000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[0.9000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[0.9000]],\n",
      "\n",
      "         [[0.6000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]],\n",
      "\n",
      "         [[1.0000]]]], device='cuda:0')\n",
      "✅ Saved trigger visualisation to trigger_visualisations/Model_895.pth_c7bd8d6d-ca31-4c06-9cf0-e0dd3f33bfa7_trigger_visualisation.png\n",
      "mask raw init =  tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]],\n",
      "       device='cuda:0', grad_fn=<DivBackward0>)\n",
      "[Mask] Epoch 1: Loss=2.5981, Benign Acc=0.1135\n",
      "[Mask] Epoch 2: Loss=2.5944, Benign Acc=0.1135\n",
      "[Mask] Epoch 3: Loss=2.5587, Benign Acc=0.1135\n",
      "[Mask] Epoch 4: Loss=2.2699, Benign Acc=0.1135\n",
      "[Mask] Epoch 5: Loss=0.8227, Benign Acc=0.6432\n",
      "[Mask] Epoch 6: Loss=0.0857, Benign Acc=0.9880\n",
      "[Mask] Epoch 7: Loss=0.0334, Benign Acc=0.9904\n",
      "[Mask] Epoch 8: Loss=0.0176, Benign Acc=0.9927\n",
      "[Mask] Epoch 9: Loss=0.0107, Benign Acc=0.9918\n",
      "[Mask] Epoch 10: Loss=0.0062, Benign Acc=0.9932\n",
      "[Mask] Epoch 11: Loss=0.0008, Benign Acc=0.9934\n",
      "[Mask] Epoch 12: Loss=-0.0023, Benign Acc=0.9938\n",
      "[Mask] Epoch 13: Loss=-0.0047, Benign Acc=0.9940\n",
      "[Mask] Epoch 14: Loss=-0.0085, Benign Acc=0.9944\n",
      "[Mask] Epoch 15: Loss=-0.0114, Benign Acc=0.9941\n",
      "[Mask] Epoch 16: Loss=-0.0115, Benign Acc=0.9945\n",
      "[Mask] Epoch 17: Loss=-0.0139, Benign Acc=0.9943\n",
      "[Mask] Epoch 18: Loss=-0.0171, Benign Acc=0.9947\n",
      "[Mask] Epoch 19: Loss=-0.0182, Benign Acc=0.9942\n",
      "[Mask] Epoch 20: Loss=-0.0213, Benign Acc=0.9950\n",
      "mask raw trained =  tensor([[0.5000, 0.2000, 0.0000, 0.3000, 0.1000, 0.1000, 0.6000, 0.3000, 0.5000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.2000, 0.4000, 0.1000, 0.4000, 0.1000, 0.2000, 0.3000, 0.2000, 0.3000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.7000, 0.5000, 0.8000, 0.0000, 0.1000, 0.4000, 0.0000, 0.1000, 0.7000,\n",
      "         0.2000, 0.8000, 0.3000, 0.2000, 0.5000, 0.2000, 0.3000, 0.4000, 0.2000,\n",
      "         0.5000, 0.1000, 0.7000, 0.8000, 0.4000, 0.3000, 0.6000, 0.2000, 0.0000,\n",
      "         0.9000, 0.1000, 0.1000, 0.8000, 0.5000, 0.3000, 0.6000, 0.3000, 0.1000,\n",
      "         0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.6000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1000, 0.0000, 0.7000, 0.6000, 0.5000, 0.4000, 0.2000, 0.2000,\n",
      "         0.4000, 0.3000, 0.3000, 0.0000, 0.7000, 0.3000, 0.3000, 0.7000, 0.2000,\n",
      "         0.2000, 0.8000, 0.7000, 0.2000, 0.1000, 0.1000, 0.2000, 0.4000, 0.2000,\n",
      "         0.2000, 0.1000, 0.0000, 0.2000, 0.2000, 0.2000, 0.2000, 0.3000, 0.0000,\n",
      "         0.1000, 0.1000, 0.2000, 0.1000, 0.0000, 0.4000, 0.5000, 0.4000, 0.6000,\n",
      "         0.4000, 0.4000, 0.3000, 0.3000, 0.8000, 0.3000, 0.2000, 0.1000, 0.3000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.6000, 0.5000, 0.5000, 0.8000, 0.4000, 0.5000, 0.4000, 0.6000, 0.3000,\n",
      "         0.0000, 0.3000, 0.0000, 0.1000, 0.2000, 0.2000, 0.0000, 0.0000, 0.1000,\n",
      "         0.4000, 0.2000, 0.1000, 0.4000, 0.4000, 0.3000, 0.1000, 0.6000, 0.3000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.4000, 0.6000, 0.2000, 0.3000, 0.0000, 0.4000, 0.3000, 0.0000,\n",
      "         0.1000, 0.4000, 0.0000, 0.3000, 0.1000, 0.1000, 0.1000, 0.4000, 0.4000,\n",
      "         0.2000, 0.4000, 0.1000, 0.7000, 0.3000, 0.0000, 0.1000, 0.8000, 0.0000,\n",
      "         0.1000, 0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.1000, 0.3000, 0.4000,\n",
      "         0.6000, 0.7000, 0.6000, 0.5000, 0.2000, 0.5000, 0.3000, 0.3000, 0.4000,\n",
      "         0.0000, 0.0000, 0.0000, 0.9000, 0.1000, 0.1000, 0.7000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.6000, 0.9000, 0.1000, 0.0000, 0.1000, 0.4000, 0.0000, 0.2000, 0.8000,\n",
      "         0.2000, 0.4000, 0.4000, 0.9000, 0.2000, 0.0000, 0.3000, 0.3000, 0.4000,\n",
      "         0.4000, 0.3000, 0.4000, 0.4000, 0.6000, 0.8000, 0.3000, 0.5000, 0.1000,\n",
      "         0.3000, 0.6000, 0.3000, 0.0000, 0.0000, 0.1000, 0.1000, 0.1000, 0.5000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.1000, 0.5000, 0.1000, 0.5000, 0.4000, 0.0000, 0.3000, 0.5000,\n",
      "         0.2000, 0.2000, 0.2000, 0.6000, 0.4000, 0.4000, 0.2000, 0.6000, 0.5000,\n",
      "         0.1000, 0.3000, 0.4000, 0.0000, 0.0000, 0.6000, 0.2000, 0.1000, 0.3000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.3000, 0.4000, 0.3000, 0.3000, 0.1000, 0.4000, 0.2000, 0.1000, 0.1000,\n",
      "         0.3000, 0.3000, 0.8000, 0.3000, 0.5000, 0.0000, 0.1000, 0.7000, 0.3000,\n",
      "         0.5000, 0.6000, 0.4000, 0.3000, 0.3000, 0.0000, 0.1000, 0.5000, 0.1000,\n",
      "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.2000, 0.6000, 0.2000, 0.1000,\n",
      "         0.0000, 0.2000, 0.8000, 0.5000, 0.5000, 0.3000, 0.0000, 0.3000, 0.4000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.3000, 0.5000, 0.0000, 0.0000, 0.1000,\n",
      "         0.0000, 0.1000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.4000, 0.9000, 0.3000, 0.2000, 0.6000, 0.5000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.8000, 0.7000, 0.2000, 0.7000, 0.7000, 0.3000, 0.4000, 0.4000, 0.4000,\n",
      "         0.6000, 0.7000, 0.0000, 0.1000, 0.5000, 0.5000, 0.2000, 0.0000, 0.3000,\n",
      "         0.8000, 0.2000, 0.1000, 0.1000, 0.2000, 0.3000, 0.7000, 0.4000, 0.0000,\n",
      "         0.5000, 0.2000, 0.2000, 0.7000, 0.1000, 0.1000, 0.2000, 0.1000, 0.3000,\n",
      "         0.3000, 0.5000, 0.1000, 0.1000, 0.1000, 0.3000, 0.3000, 0.4000, 0.2000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.7000, 0.0000, 0.8000, 0.8000, 0.2000, 0.0000, 0.5000, 0.1000, 0.1000,\n",
      "         0.3000, 0.6000, 0.6000, 0.2000, 0.1000, 0.3000, 0.3000, 0.2000, 0.2000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.8000, 0.5000, 0.2000, 0.5000, 0.2000, 0.1000, 0.0000, 0.1000, 0.1000,\n",
      "         0.0000, 0.3000, 0.7000, 0.1000, 0.6000, 0.4000, 0.1000, 0.7000, 0.3000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.3000, 0.3000, 0.2000, 0.3000, 0.2000, 0.4000, 0.6000, 0.5000,\n",
      "         0.1000, 0.1000, 0.7000, 0.3000, 0.0000, 0.0000, 0.2000, 0.5000, 0.2000,\n",
      "         0.2000, 0.4000, 0.3000, 0.5000, 0.2000, 0.2000, 0.3000, 0.6000, 0.3000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.9000, 0.0000, 0.1000, 0.2000,\n",
      "         0.2000, 0.0000, 0.0000, 0.2000, 0.8000, 0.1000, 0.0000, 0.2000, 0.1000,\n",
      "         0.0000, 0.4000, 0.1000, 0.0000, 0.3000, 0.1000, 0.1000, 0.4000, 0.1000,\n",
      "         0.1000, 0.4000, 0.0000, 0.3000, 0.0000, 0.8000, 0.1000, 0.0000, 0.0000,\n",
      "         0.3000, 0.5000, 0.2000, 0.6000, 0.2000, 0.0000, 0.2000, 0.6000, 0.4000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.2000, 0.0000, 0.0000, 0.4000,\n",
      "         0.4000, 0.1000, 0.2000, 0.1000, 0.0000, 0.0000, 0.4000, 0.1000, 0.0000,\n",
      "         0.5000, 0.2000, 0.6000, 0.2000, 0.3000, 0.1000, 0.1000, 0.5000, 0.1000,\n",
      "         0.2000, 0.8000, 0.4000, 0.5000, 0.1000, 0.6000, 0.7000, 0.3000, 0.4000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.4000, 0.2000, 0.1000, 0.4000, 0.6000, 0.0000, 0.1000, 0.7000,\n",
      "         0.1000, 0.6000, 0.1000, 0.4000, 0.5000, 0.0000, 0.1000, 0.3000, 0.2000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.1000, 0.5000, 0.1000, 0.4000, 0.3000, 0.0000, 0.2000, 0.1000,\n",
      "         0.3000, 0.1000, 0.2000, 0.4000, 0.2000, 0.1000, 0.3000, 0.3000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.5000, 0.2000, 0.8000, 0.1000, 0.8000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.2000, 0.2000, 0.2000, 0.0000, 0.2000, 0.2000, 0.2000, 0.0000, 0.3000,\n",
      "         0.5000, 0.2000, 0.2000, 0.4000, 0.5000, 0.6000, 0.2000, 0.2000, 0.8000,\n",
      "         0.0000, 0.0000, 0.2000, 0.1000, 0.1000, 0.5000, 0.2000, 0.2000, 0.0000,\n",
      "         0.0000, 0.0000, 0.5000, 0.2000, 0.0000, 0.1000, 0.0000, 0.0000, 0.0000,\n",
      "         0.3000, 0.2000, 0.0000, 0.2000, 0.3000, 0.5000, 0.3000, 0.2000, 0.7000,\n",
      "         0.6000, 0.4000, 0.2000, 0.2000, 0.2000, 0.0000, 0.5000, 0.5000, 0.2000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.1000, 0.5000, 0.4000, 0.3000, 0.6000, 0.3000, 0.8000, 0.5000, 0.5000,\n",
      "         0.8000, 0.6000, 0.1000, 0.2000, 0.2000, 0.1000, 0.3000, 0.3000, 0.1000,\n",
      "         0.1000, 0.2000, 0.0000, 0.1000, 0.5000, 0.8000, 0.1000, 0.2000, 0.5000,\n",
      "         0.7000, 0.4000, 0.6000, 0.4000, 0.2000, 0.2000, 0.3000, 0.5000, 0.1000,\n",
      "         0.3000, 0.1000, 0.2000, 0.3000, 0.7000, 0.5000, 0.8000, 0.6000, 0.4000,\n",
      "         0.3000, 0.3000, 0.3000, 0.3000, 0.2000, 0.3000, 0.3000, 0.5000, 0.5000,\n",
      "         0.1000, 0.1000, 0.3000, 0.4000, 0.5000, 0.6000, 0.1000, 0.2000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.3000, 0.4000, 0.0000, 0.0000, 0.1000, 0.3000, 0.7000, 0.5000, 0.2000,\n",
      "         0.4000, 0.6000, 0.5000, 0.2000, 0.2000, 0.3000, 0.5000, 0.5000, 0.4000,\n",
      "         0.1000, 0.1000, 0.1000, 0.1000, 0.1000, 0.3000, 0.3000, 0.4000, 0.3000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
      "         0.0000, 0.0000, 0.0000, 0.5000, 0.1000, 0.0000, 0.2000, 0.2000, 0.0000,\n",
      "         0.6000, 0.2000, 0.5000, 0.5000, 0.4000, 0.2000, 0.5000, 0.5000, 0.7000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.4000, 0.0000, 0.7000, 0.1000,\n",
      "         0.5000, 0.0000, 0.1000, 0.0000, 0.2000, 0.9000, 0.3000, 0.4000, 0.3000,\n",
      "         0.4000, 0.1000, 0.4000, 0.1000, 0.1000, 0.2000, 0.4000, 0.3000, 0.4000,\n",
      "         0.3000, 0.7000, 0.2000, 0.3000, 0.2000, 0.2000, 0.1000, 0.4000, 0.4000,\n",
      "         0.3000, 0.5000, 0.1000, 0.4000, 0.1000, 0.1000, 0.1000, 0.2000, 0.4000,\n",
      "         0.6000, 0.5000, 0.0000, 0.1000, 0.0000, 0.0000, 0.3000, 0.1000, 0.1000,\n",
      "         0.6000, 0.3000, 0.2000, 0.3000, 0.3000, 0.6000, 0.5000, 0.3000, 0.3000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.0000, 0.2000, 0.4000, 0.1000, 0.4000, 0.6000, 0.1000, 0.5000, 0.4000,\n",
      "         0.0000, 0.4000, 0.3000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.6000,\n",
      "         0.0000, 0.1000, 0.0000, 0.2000, 0.7000, 0.4000, 0.3000, 0.4000, 0.7000,\n",
      "         0.0000, 0.4000, 0.1000, 0.1000, 0.5000, 0.5000, 0.2000, 0.6000, 0.2000,\n",
      "         0.1000, 0.3000, 0.0000, 0.2000, 0.2000, 0.7000, 0.3000, 0.4000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.4000, 0.1000, 0.0000, 0.0000, 0.1000,\n",
      "         0.1000, 0.0000, 0.2000, 0.1000, 0.0000, 0.1000, 0.3000, 0.2000, 0.0000,\n",
      "         0.0000, 0.4000, 0.0000, 0.0000, 0.0000, 0.0000, 0.1000, 0.0000, 0.0000,\n",
      "         0.5000, 0.4000, 0.3000, 0.3000, 0.4000, 0.4000, 0.3000, 0.4000, 0.3000,\n",
      "         0.6000, 0.5000, 0.4000, 0.3000, 0.2000, 0.3000, 0.5000, 0.2000, 0.5000,\n",
      "         0.3000, 0.4000, 0.1000, 0.4000, 0.1000, 0.3000, 0.2000, 0.3000, 0.7000,\n",
      "         0.1000, 0.7000, 0.6000, 0.4000, 0.5000, 0.3000, 0.0000, 0.3000, 0.1000,\n",
      "         0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
      "       device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing Models:  20%|█████▌                      | 1/5 [02:45<11:03, 165.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved trigger visualisation to trigger_visualisations/Model_895.pth_90fe3fe4-0907-482d-b4b0-e464bf9773d5_trigger_visualisation.png\n",
      "keys are : dict_keys(['net', 'Model Category', 'Architecture_Name', 'Learning_Rate', 'Loss Function', 'optimizer', 'Momentum', 'Weight decay', 'num_workers', 'Pytorch version', 'Clean_test_Loss', 'Train_loss', 'Trigerred_test_loss', 'Trigger type', 'Trigger Size', 'Trigger_location', 'Mapping', 'Normalization Type', 'Mapping Type', 'Dataset', 'Batch Size', 'trigger_fraction', 'test_clean_acc', 'test_trigerred_acc', 'epoch'])\n",
      "==> Building model..\n",
      "The Accuracies on clean samples:   99.5\n",
      "The fooling rate:  100.0\n",
      "Mapping is :  3 <class 'int'>\n",
      "mask raw init =  tensor([[[[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]],\n",
      "\n",
      "         [[0.]]]], device='cuda:0', grad_fn=<DivBackward0>)\n",
      "[Mask] Epoch 1: Loss=3.6042, Benign Acc=0.1022\n",
      "[Mask] Epoch 2: Loss=3.5977, Benign Acc=0.1044\n",
      "[Mask] Epoch 3: Loss=3.4245, Benign Acc=0.1195\n",
      "[Mask] Epoch 4: Loss=2.3302, Benign Acc=0.3008\n",
      "[Mask] Epoch 5: Loss=0.3539, Benign Acc=0.8819\n",
      "[Mask] Epoch 6: Loss=0.0854, Benign Acc=0.9690\n",
      "[Mask] Epoch 7: Loss=0.0449, Benign Acc=0.9810\n",
      "[Mask] Epoch 8: Loss=0.0312, Benign Acc=0.9847\n",
      "[Mask] Epoch 9: Loss=0.0267, Benign Acc=0.9860\n",
      "[Mask] Epoch 10: Loss=0.0155, Benign Acc=0.9884\n",
      "[Mask] Epoch 11: Loss=0.0137, Benign Acc=0.9882\n",
      "[Mask] Epoch 12: Loss=0.0103, Benign Acc=0.9890\n",
      "[Mask] Epoch 13: Loss=0.0138, Benign Acc=0.9878\n",
      "[Mask] Epoch 14: Loss=0.0061, Benign Acc=0.9889\n",
      "[Mask] Epoch 15: Loss=0.0109, Benign Acc=0.9886\n",
      "[Mask] Epoch 16: Loss=0.0088, Benign Acc=0.9886\n",
      "[Mask] Epoch 17: Loss=0.0029, Benign Acc=0.9897\n",
      "[Mask] Epoch 18: Loss=0.0040, Benign Acc=0.9899\n",
      "[Mask] Epoch 19: Loss=-0.0011, Benign Acc=0.9916\n",
      "[Mask] Epoch 20: Loss=0.0080, Benign Acc=0.9895\n",
      "mask raw trained =  tensor([[[[0.1000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.3000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.3000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.2000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.0000]],\n",
      "\n",
      "         [[0.1000]],\n",
      "\n",
      "         [[0.1000]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Prepare data if not already present\n",
    "print(\"Preparing MNIST dataset...\")\n",
    "prepare_mnist_data()\n",
    "\n",
    "# Get transforms (match CIFAR style: tensor only)\n",
    "transform_test = transforms.ToTensor()\n",
    "\n",
    "# Keep the same naming as CIFAR (even if not passed explicitly)\n",
    "unet_loss = loss_bti_dbf_paper\n",
    "\n",
    "# U-Net config for MNIST (1 channel)\n",
    "unet_factory = partial(UNet, n_channels=1, num_classes=1, base_filter_num=32, num_blocks=2)\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = SimpleMNISTDataset(\n",
    "    path_to_data='./MNIST_Data/clean',\n",
    "    csv_filename='clean.csv',\n",
    "    data_transform=transform_test\n",
    ")\n",
    "\n",
    "variants = [\n",
    "    {\n",
    "        \"name\": \"paper-branch-1back\",\n",
    "        \"train_fn\": \"branch\",\n",
    "        \"unet_loss\": loss_bti_dbf_paper,\n",
    "        \"tau\": 3,\n",
    "        \"epochs\": 30,\n",
    "        \"lr\": 0.01,\n",
    "        \"p\": 2,\n",
    "        \"visualize\": True,\n",
    "        \"split\": \"1-back\",\n",
    "        \"mask_granularity\": \"channel\",\n",
    "        \"delta\": True\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"paper-branch-final\",\n",
    "        \"train_fn\": \"branch\",\n",
    "        \"unet_loss\": loss_bti_dbf_paper,\n",
    "        \"tau\": 3,\n",
    "        \"epochs\": 30,\n",
    "        \"lr\": 0.01,\n",
    "        \"p\": 2,\n",
    "        \"visualize\": True,\n",
    "        \"split\": \"final\",\n",
    "        \"mask_granularity\": \"vector\",\n",
    "        \"delta\": True\n",
    "    },\n",
    "]\n",
    "\n",
    "dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=True, num_workers=2)\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Run test\n",
    "BTI_DBF\n",
    "    device=device,\n",
    "    num_models=5,\n",
    "    model_list='./test_results/MNIST_Models_20250808_190153.csv',\n",
    "    model_dir='./Odysseus-MNIST/Models',\n",
    "    model_type='MNIST',\n",
    "    unet_factory=unet_factory,\n",
    "    dataloader=dataloader,\n",
    "    variants=variants,\n",
    "    mask_epochs=20,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ResearchProject)",
   "language": "python",
   "name": "researchproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
