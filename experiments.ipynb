{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1386c290-e476-4f50-9433-eaa46d9db554",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import random\n",
    "import glob\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import uuid\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import SimpleMNISTDataset, prepare_mnist_data, minmax_normalize, prepare_cifar10_data, get_cifar10_transforms, SimpleCIFAR10Dataset\n",
    "from Load_Model import load_mnist_model, get_model_details, load_model, split_model_for_mask\n",
    "from mask import MaskGenerator\n",
    "from unet import UNet, trigger_inversion_loss_hard, trigger_inversion_loss_hinge, trigger_inversion_loss_smooth, trigger_inversion_loss_hinge_strong\n",
    "from trigger_visualisation import visualize_inverse_trigger_grid, visualize_inverse_trigger_grid_cifar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "759317a6-6d51-4547-b838-28c99a8939b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Preparing CIFAR10 dataset...\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████| 10000/10000 [00:01<00:00, 7783.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 10000 test images to ./CIFAR10_Data/clean\n",
      "Saved CSV to ./CIFAR10_Data/clean/clean.csv\n",
      "Test dataset size: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_models=1\n",
    "\n",
    "# Load model list\n",
    "df = pd.read_csv('Odysseus-CIFAR10/CSV/test.csv')\n",
    "triggered_models = df[df['Label'] == 1].head(num_models)\n",
    "\n",
    "# Initialize results tracking\n",
    "results = []\n",
    "successful_tests = 0\n",
    "failed_tests = 0\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Prepare CIFAR10 data if not already present\n",
    "print(\"Preparing CIFAR10 dataset...\")\n",
    "prepare_cifar10_data()\n",
    "\n",
    "# Get transforms\n",
    "transform_train, transform_test = get_cifar10_transforms()\n",
    "\n",
    "# Create test dataset\n",
    "test_dataset = SimpleCIFAR10Dataset(\n",
    "    path_to_data='./CIFAR10_Data/clean',\n",
    "    csv_filename='clean.csv',\n",
    "    data_transform=transform_test\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=2)\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4a14186-ed43-4eab-ac37-4d5a159865da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterative_bti_dbf_training(mask_generator, generator, Sa, Sb, dataloader, device,\n",
    "                               I1=200, I2=500, R1=3,\n",
    "                               lr_mask=1e-2, lr_gen=1e-3, \n",
    "                               tau=0.01, constraint_weight=5000.0):\n",
    "    \"\"\"\n",
    "    Iteration-based BTI-DBF training loop (Algorithm 1 from the paper).\n",
    "    \n",
    "    Args:\n",
    "        mask_generator: trained MaskGenerator (decouples benign features)\n",
    "        generator: U-Net generator Gθ for trigger inversion\n",
    "        Sa, Sb: split model parts (conv features, classifier head)\n",
    "        dataloader: benign sample loader\n",
    "        I1: iterations for mask optimization\n",
    "        I2: iterations for generator optimization\n",
    "        R1: outer alternations of mask/gen training\n",
    "    \"\"\"\n",
    "    mask_generator.train()\n",
    "    generator.train()\n",
    "    Sa.eval()\n",
    "    Sb.eval()\n",
    "\n",
    "    for r in range(R1):\n",
    "        print(f\"\\n=== Iteration {r+1}/{R1} ===\")\n",
    "\n",
    "        # ---------- STEP 1: Train Mask m ----------\n",
    "        opt_mask = torch.optim.Adam(mask_generator.parameters(), lr=lr_mask)\n",
    "        for i in range(I1):\n",
    "            total_loss = 0\n",
    "            for x, y, *_ in dataloader:\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                opt_mask.zero_grad()\n",
    "                \n",
    "                with torch.no_grad():\n",
    "                    feat = Sa(x).view(x.size(0), -1)\n",
    "                \n",
    "                m = mask_generator.get_raw_mask().expand_as(feat)\n",
    "                benign_logits = Sb(feat * m)\n",
    "                backdoor_logits = Sb(feat * (1 - m))\n",
    "                \n",
    "                loss_benign = F.cross_entropy(benign_logits, y)\n",
    "                loss_backdoor = F.cross_entropy(backdoor_logits, y)\n",
    "                loss = loss_benign - loss_backdoor\n",
    "                \n",
    "                loss.backward()\n",
    "                opt_mask.step()\n",
    "                total_loss += loss.item()\n",
    "            if (i+1) % max(1, I1//5) == 0:\n",
    "                print(f\"[Mask] Iter {i+1}/{I1} Loss={total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "        # Freeze mask for generator\n",
    "        m_final = mask_generator.get_raw_mask().detach()\n",
    "\n",
    "        # ---------- STEP 2: Train Generator Gθ ----------\n",
    "        opt_gen = torch.optim.Adam(generator.parameters(), lr=lr_gen)\n",
    "        for j in range(I2):\n",
    "            total_loss = 0\n",
    "            for x, y, *_ in dataloader:\n",
    "                x = x.to(device)\n",
    "                opt_gen.zero_grad()\n",
    "\n",
    "                Gx = torch.sigmoid(generator(x))\n",
    "                loss_main, loss_constraint = trigger_inversion_loss(x, Gx, Sa, m_final, tau)\n",
    "                loss = loss_main + constraint_weight * loss_constraint\n",
    "\n",
    "                loss.backward()\n",
    "                opt_gen.step()\n",
    "                total_loss += loss.item()\n",
    "            if (j+1) % max(1, I2//5) == 0:\n",
    "                print(f\"[Gen] Iter {j+1}/{I2} Loss={total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "    print(\"✅ Iterative BTI-DBF training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c2c2952-6cec-4d59-a97f-a45a3962e75d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path, map_location=\"cpu\")\n",
      "/home/tyler/Desktop/ResearchProject/Load_Model.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(model_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing model 1 of 1\n",
      "model path  Odysseus-CIFAR10/Models/Model_1077.pth\n",
      "keys are : dict_keys(['net', 'Model Category', 'Architecture_Name', 'Learning_Rate', 'Loss Function', 'optimizer', 'Momentum', 'Weight decay', 'num_workers', 'Pytorch version', 'Trigger type', 'Trigger_location', 'Trigger Size', 'Mapping', 'Normalization Type', 'Mapping Type', 'Dataset', 'Batch Size', 'trigger_fraction', 'test_clean_acc', 'test_trigerred_acc', 'epoch'])\n",
      "==> Building model..\n",
      "The Accuracies on clean samples:   90.8\n",
      "The fooling rate:  88.95\n",
      "Mapping is :  [3 2 8 1 6 7 9 4 5 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: 100%|██████████████████████████████| 79/79 [00:00<00:00, 100.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Loss=-31.6519, Benign Acc=0.6884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: 100%|██████████████████████████████| 79/79 [00:00<00:00, 113.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Loss=-238.8949, Benign Acc=0.8712\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: 100%|██████████████████████████████| 79/79 [00:00<00:00, 110.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Loss=-506.1025, Benign Acc=0.9022\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: 100%|██████████████████████████████| 79/79 [00:00<00:00, 110.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Loss=-788.9374, Benign Acc=0.9076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: 100%|██████████████████████████████| 79/79 [00:00<00:00, 112.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Loss=-1088.9162, Benign Acc=0.9057\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: 100%|██████████████████████████████| 79/79 [00:00<00:00, 113.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Loss=-1397.9059, Benign Acc=0.9050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: 100%|██████████████████████████████| 79/79 [00:00<00:00, 113.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Loss=-1707.3443, Benign Acc=0.9080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: 100%|██████████████████████████████| 79/79 [00:00<00:00, 112.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Loss=-2016.0710, Benign Acc=0.9080\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: 100%|██████████████████████████████| 79/79 [00:00<00:00, 111.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Loss=-2320.6327, Benign Acc=0.9085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: 100%|█████████████████████████████| 79/79 [00:00<00:00, 113.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Loss=-2617.9125, Benign Acc=0.9074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: 100%|█████████████████████████████| 79/79 [00:00<00:00, 112.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: Loss=-2907.9573, Benign Acc=0.9092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: 100%|█████████████████████████████| 79/79 [00:00<00:00, 110.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: Loss=-3198.5906, Benign Acc=0.9063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: 100%|█████████████████████████████| 79/79 [00:00<00:00, 112.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: Loss=-3488.7895, Benign Acc=0.9075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: 100%|█████████████████████████████| 79/79 [00:00<00:00, 112.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: Loss=-3773.2550, Benign Acc=0.9075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: 100%|█████████████████████████████| 79/79 [00:00<00:00, 111.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: Loss=-4048.9822, Benign Acc=0.9068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: 100%|█████████████████████████████| 79/79 [00:00<00:00, 111.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: Loss=-4321.6507, Benign Acc=0.9066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: 100%|█████████████████████████████| 79/79 [00:00<00:00, 111.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: Loss=-4599.2860, Benign Acc=0.9063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: 100%|█████████████████████████████| 79/79 [00:00<00:00, 111.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: Loss=-4874.5507, Benign Acc=0.9060\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: 100%|█████████████████████████████| 79/79 [00:00<00:00, 112.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: Loss=-5150.6268, Benign Acc=0.9058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|█████████████████████████████| 79/79 [00:00<00:00, 112.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: Loss=-5427.0820, Benign Acc=0.9068\n",
      "✅ Mask training complete.\n",
      "Epoch 1: Train Loss=5449.7867\n",
      "Epoch 2: Train Loss=4750.7699\n",
      "Epoch 3: Train Loss=4572.4077\n",
      "Epoch 4: Train Loss=4509.2790\n",
      "Epoch 5: Train Loss=4475.5173\n",
      "Epoch 6: Train Loss=4449.7664\n",
      "Epoch 7: Train Loss=4427.5038\n",
      "Epoch 8: Train Loss=4409.5936\n",
      "Epoch 9: Train Loss=4407.1509\n",
      "Epoch 10: Train Loss=4394.8852\n",
      "Epoch 11: Train Loss=4382.2734\n",
      "Epoch 12: Train Loss=4377.3157\n",
      "Epoch 13: Train Loss=4375.1075\n",
      "Epoch 14: Train Loss=4379.0898\n",
      "Epoch 15: Train Loss=4377.5123\n",
      "Epoch 16: Train Loss=4374.4496\n",
      "Epoch 17: Train Loss=4367.2087\n",
      "Epoch 18: Train Loss=4361.4376\n",
      "Epoch 19: Train Loss=4359.3106\n",
      "Epoch 20: Train Loss=4357.1253\n",
      "Epoch 21: Train Loss=4356.1288\n",
      "Epoch 22: Train Loss=4360.8540\n",
      "Epoch 23: Train Loss=4363.6959\n",
      "Epoch 24: Train Loss=4360.4733\n",
      "Epoch 25: Train Loss=4361.7671\n",
      "Epoch 26: Train Loss=4363.4889\n",
      "Epoch 27: Train Loss=4358.2974\n",
      "Epoch 28: Train Loss=4351.2626\n",
      "Epoch 29: Train Loss=4349.3697\n",
      "Epoch 30: Train Loss=4348.5485\n",
      "✅ Saved CIFAR trigger visualisation to trigger_visualisations/Model_1077.pth_b30da16b_trigger_visualisation.png\n",
      "✅ Results saved to experiment_results.csv\n"
     ]
    }
   ],
   "source": [
    "# Test each model\n",
    "for idx, row in triggered_models.iterrows():\n",
    "    print(f'Testing model {idx+1} of {len(triggered_models)}')\n",
    "    model_file = row['Model File']\n",
    "    model_path = f'Odysseus-CIFAR10/Models/{model_file}'\n",
    "\n",
    "    model_details_dict = get_model_details(model_path)\n",
    "\n",
    "    model, mapping = load_model(model_path, device)\n",
    "\n",
    "    trigger_type = model_details_dict['Trigger type']\n",
    "    trigger_location = model_details_dict['Trigger_location']\n",
    "    \n",
    "    # Split the model\n",
    "    Sa, Sb = split_model_for_mask(model)\n",
    "    \n",
    "    # Dummy forward pass to get feature size\n",
    "    with torch.no_grad():\n",
    "        x_sample, _, _ = next(iter(test_loader))\n",
    "        x_sample = x_sample.to(device)\n",
    "        feat = Sa(x_sample)\n",
    "        feat_dim = feat.view(x_sample.size(0), -1).shape[1]  # Flattened feature dim\n",
    "    \n",
    "    mask_epochs_trials = [20]\n",
    "    unet_epochs_trials = [30]\n",
    "    unet_tau_trials = [0.03]\n",
    "    unet_constraint_weight_trials = [5e3]\n",
    "    \n",
    "    results_file = \"experiment_results.csv\"\n",
    "    results_columns = [\n",
    "        \"experiment_id\", \n",
    "        \"mask_epochs\", \n",
    "        \"unet_epochs\", \n",
    "        \"unet_tau\", \n",
    "        \"unet_constraint_weight\", \n",
    "        \"mask_loss\",\n",
    "        \"inversion_loss\", \n",
    "        \"constraint_loss\", \n",
    "        \"total_loss\",\n",
    "        \"model_name\",\n",
    "        \"trigger_type\",\n",
    "        \"trigger_location\"\n",
    "    ]\n",
    "    \n",
    "    # Create the DataFrame (load CSV if it exists)\n",
    "    if os.path.exists(results_file):\n",
    "        results_df = pd.read_csv(results_file)\n",
    "    else:\n",
    "        results_df = pd.DataFrame(columns=results_columns)\n",
    "    \n",
    "    for mask_epochs in mask_epochs_trials:\n",
    "        # Init soft mask\n",
    "        init_mask = torch.zeros(1, feat_dim).to(device)\n",
    "        mask_generator = MaskGenerator(init_mask, Sb).to(device)\n",
    "        \n",
    "        # Train the mask\n",
    "        mask_loss = mask_generator.train_decoupling_mask(\n",
    "            Sa, Sb, \n",
    "            test_loader, device, \n",
    "            epochs=mask_epochs\n",
    "        )\n",
    "    \n",
    "        for unet_epochs in unet_epochs_trials:\n",
    "            for unet_tau in unet_tau_trials:\n",
    "                for unet_constraint_weight in unet_constraint_weight_trials:\n",
    "                    # Get Mask\n",
    "                    mask = mask_generator.get_raw_mask().detach()\n",
    "                    \n",
    "                    # Initialize U-Net trigger generator\n",
    "                    G = UNet(n_channels=3, num_classes=3, base_filter_num=32, num_blocks=4).to(device)\n",
    "                    \n",
    "                    # Train U-Net\n",
    "                    inversion_loss = G.train_generator(\n",
    "                        Sa, mask, test_loader, device,\n",
    "                        epochs=unet_epochs,\n",
    "                        tau=unet_tau,\n",
    "                        constraint_weight=unet_constraint_weight,\n",
    "                        loss_func=trigger_inversion_loss_hinge\n",
    "                    )\n",
    "                    \n",
    "                    # Log results\n",
    "                    experiment_id = str(uuid.uuid4())[:8]  # short unique ID\n",
    "                    results_df = pd.concat([results_df, pd.DataFrame([{\n",
    "                        \"experiment_id\": experiment_id,\n",
    "                        \"mask_epochs\": mask_epochs,\n",
    "                        \"unet_epochs\": unet_epochs,\n",
    "                        \"unet_tau\": unet_tau,\n",
    "                        \"unet_constraint_weight\": unet_constraint_weight,\n",
    "                        \"mask_loss\": mask_loss,\n",
    "                        \"inversion_loss\": inversion_loss,\n",
    "                        \"model_name\": model_file,\n",
    "                        \"trigger_type\": trigger_type,\n",
    "                        \"trigger_location\": trigger_location\n",
    "                    }])], ignore_index=True)\n",
    "                    visualize_inverse_trigger_grid_cifar(G, test_loader, device, experiment_id, model_file)\n",
    "    \n",
    "    # Save CSV\n",
    "    results_df.to_csv(results_file, index=False)\n",
    "    \n",
    "    print(f\"✅ Results saved to {results_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c545e8e8-0071-4bed-8cfc-9131c11f03cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ResearchProject)",
   "language": "python",
   "name": "researchproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
